{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7e1d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the module: helpers.mpl_plotting_helpers\n",
      "\n",
      "Loading the module: helpers.general_helpers\n",
      "\n",
      "Loading the module: helpers.argcheck_helpers\n",
      "\n",
      "Loading the module: helpers.pandas_helpers\n",
      "\n",
      "Loading the module: helpers.stats_helpers.py\n",
      "\n",
      "numpy        1.22.4\n",
      "scipy         1.8.1\n",
      "pandas        1.4.2\n",
      "\n",
      "pandas        1.4.2\n",
      "numpy         1.22.4\n",
      "\n",
      "matplotlib    3.5.2\n",
      "numpy         1.22.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "#\n",
    "# Imporatbles, just the basics :P\n",
    "\n",
    "    # My module, cuz I'm cool\n",
    "from helpers import mpl_plotting_helpers as mph\n",
    "from helpers import stats_helpers as sh\n",
    "from helpers import general_helpers as gh\n",
    "from helpers import western_helpers as wh\n",
    "from helpers.mph_modules.dotplots import get_data_info, add_errorbar\n",
    "\n",
    "    # Standard packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as mpl_fm\n",
    "from math import floor, ceil, log2\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#\n",
    "#\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbae10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "#\n",
    "# Making line plots and doing the statistics\n",
    "#  #  #  # Add to mpl_plotting_helpers at some point\n",
    "\n",
    "def _logical_ignore_comps(labelled_line_groups,\n",
    "                          group_strs,\n",
    "                          xgroup_strs):\n",
    "    \"\"\"\n",
    "    Only want to compare along a line group (e.g. timecourse) or\n",
    "    down an x-column (e.g. JE6 DMSO 0m vs JE6 U0126 0m), but not\n",
    "    all the random other comparisons because statistically they're\n",
    "    kind of useless\n",
    "    \n",
    "    So this function will find all of the pairs that are useless\n",
    "    \"\"\"\n",
    "    groups_unpacked = []\n",
    "    for group in labelled_line_groups:\n",
    "        groups_unpacked += group\n",
    "    # This will hold the ignored pairs\n",
    "    ignore_me_senpai = []\n",
    "    # First, get all pairs\n",
    "    paired = gh.make_pairs(groups_unpacked,\n",
    "                           dupes = False,\n",
    "                           reverse = False)\n",
    "    # Then iterate over and check the labels\n",
    "    for p in paired:\n",
    "        gs_check = 0\n",
    "        xs_check = 0\n",
    "        # Check all the group strings\n",
    "        for gs in group_strs:\n",
    "            if gs_check == 1:\n",
    "                pass\n",
    "            elif gs in p[0][0] and gs in p[1][0]:\n",
    "                gs_check = 1\n",
    "        # Check all the xgroup strings\n",
    "        for xs in xgroup_strs:\n",
    "            if xs_check == 1:\n",
    "                pass\n",
    "            elif xs in p[0][0] and xs in p[1][0]:\n",
    "                xs_check = 1\n",
    "        # If there isn't a match, in either, ignore\n",
    "        if gs_check == 0 and xs_check == 0:\n",
    "            ignore_me_senpai.append(p)\n",
    "    # Return the ignored pairs at the end\n",
    "    return ignore_me_senpai\n",
    "\n",
    "def perform_line_statistics(labelled_line_groups,\n",
    "                            ignore_comps,\n",
    "                            comp_type,\n",
    "                            statsfile):\n",
    "    \"\"\"\n",
    "    labelled_line_groups -> data with labels\n",
    "                            list of lists of [label, [d1,d2,...,dn]]\n",
    "    ignore_comps -> list of pairs (\"group 1\", \"group 2\") to not be\n",
    "                    compared\n",
    "    comp_type -> statistics to use, currently only\n",
    "                 [\"HolmSidak\", \"TukeyHSD\"] are supported\n",
    "                 (both do an ANOVA first by default)\n",
    "    statsfile -> a string to the output path and filename\n",
    "                 for the statistics file output\n",
    "    #####\n",
    "    Returns None, just dumps the statsfile\n",
    "    \"\"\"\n",
    "    assert comp_type in [\"HolmSidak\", \"TukeyHSD\"], f\"Invalid comparison type: {comp_type}\"\n",
    "    groups_unpacked = []\n",
    "    for group in labelled_line_groups:\n",
    "        groups_unpacked += group\n",
    "    if comp_type == \"HolmSidak\":\n",
    "        comparison = sh.HolmSidak(*groups_unpacked,\n",
    "                                  labels = True,\n",
    "                                  override = True,\n",
    "                                  alpha = 0.05,\n",
    "                                  no_comp = ignore_comps)\n",
    "    elif comp_type == \"TukeyHSD\":\n",
    "        comparison = sh.TukeyHSD(*groups_unpacked,\n",
    "                                  labels = True,\n",
    "                                  override = True,\n",
    "                                  alpha = 0.05,\n",
    "                                  no_comp = ignore_comps)\n",
    "    comparison.write_output(filename = statsfile,\n",
    "                            file_type = \"csv\")\n",
    "    return None\n",
    "\n",
    "def find_centres(plotting_info):\n",
    "    \"\"\"\n",
    "    plotting_info -> output from get_data_info, a list of\n",
    "                     data info and the raw data\n",
    "                     \n",
    "    goal: grab the centres for xticks\n",
    "    \"\"\"\n",
    "    centres = []\n",
    "    for group in plotting_info:\n",
    "        if len(centres) <= len(group[0][\"centers\"]):\n",
    "            centres = group[0][\"centers\"]\n",
    "    return centres\n",
    "\n",
    "def line_plot(labelled_line_groups,\n",
    "              show_points = False,\n",
    "              show_legend = False,\n",
    "              colours = [\"grey\" for _ in range(20)],\n",
    "              group_labs = [f\"Thing {i}\" for i in range(20)],\n",
    "              markers = [\"s\" for _ in range(20)],\n",
    "              linestyles = [\"solid\" for _ in range(20)],\n",
    "              xlabels = [f\"Time {i}\" for i in range(20)],\n",
    "              ylabel = [\"Fold change\"],\n",
    "              ylims = None,\n",
    "              ignore_comps = [],\n",
    "              statsfile = None,\n",
    "              comp_type = \"HolmSidak\",\n",
    "              figfile = None):\n",
    "    \"\"\"\n",
    "    labelled_line_groups -> list of lists, where each sublist contains labelled groups\n",
    "    \"\"\"\n",
    "    # First, get some basic plotting information\n",
    "    plotting_info = [get_data_info(line) for line in labelled_line_groups]\n",
    "    # Then manage the statistics\n",
    "    if statsfile != None:\n",
    "        perform_line_statistics(labelled_line_groups, \n",
    "                                ignore_comps, \n",
    "                                comp_type, \n",
    "                                statsfile)\n",
    "    # Begin plotting c::\n",
    "    if ylims == None:\n",
    "        ylims = floor(min([item for item in gh.unpack_list(labelled_line_groups) if type(item) in [int, float]])), ceil(max([item for item in gh.unpack_list(labelled_line_groups) if type(item) in [int, float]]))\n",
    "    # \n",
    "    fig, ax = plt.subplots(figsize = (6,6))\n",
    "    # \n",
    "    for i in range(len(labelled_line_groups)):\n",
    "        #\n",
    "        ax.plot(plotting_info[i][0][\"centers\"],\n",
    "                plotting_info[i][0][\"means\"],\n",
    "                color = colours[i],\n",
    "                label = group_labs[i],\n",
    "                linestyle = linestyles[i])\n",
    "        #\n",
    "        for j in range(len(labelled_line_groups[i])):\n",
    "            add_errorbar(ax, \n",
    "                         plotting_info[i][0][\"centers\"][j],\n",
    "                         plotting_info[i][0][\"means\"][j],\n",
    "                         plotting_info[i][0][\"sems\"][j],\n",
    "                         color = colours[i])\n",
    "            if show_points:\n",
    "            #\n",
    "                ax.scatter(plotting_info[i][0][\"xs\"][j],\n",
    "                           plotting_info[i][1][j][1],\n",
    "                           color = colours[i],\n",
    "                           edgecolor = \"black\", alpha = 0.3,\n",
    "                           marker = markers[i],\n",
    "                           s = 10)\n",
    "            else:\n",
    "            #\n",
    "                ax.scatter(plotting_info[i][0][\"centers\"],\n",
    "                           plotting_info[i][0][\"means\"],\n",
    "                           color = colours[i],\n",
    "                           edgecolor = \"black\", alpha = 0.3,\n",
    "                           marker = markers[i],\n",
    "                           s = 30)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    xticks = find_centres(plotting_info)\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xlabels[:len(xticks)],\n",
    "                       fontfamily = \"sans-serif\", \n",
    "                       font = \"Arial\", \n",
    "                       fontweight = \"bold\", \n",
    "                       fontsize = 12,\n",
    "                       rotation = 90,\n",
    "                       ha = \"center\")\n",
    "    ax.set_ylim(*ylims)\n",
    "    mph.update_ticks(ax, which = \"y\")\n",
    "    if show_legend:\n",
    "        ax.legend(loc = \"best\",\n",
    "                  prop = mpl_fm.FontProperties(family = \"sans-serif\",\n",
    "                                               weight = \"bold\"))\n",
    "    if figfile == None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(figfile)\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "def neg_to_zero(a_list):\n",
    "    \"\"\"\n",
    "    replace any value <0 with 0\n",
    "    \"\"\"\n",
    "    newlist = []\n",
    "    for item in a_list:\n",
    "        try:\n",
    "            truth = item < 0\n",
    "        except:\n",
    "            newlist.append(item)\n",
    "        else:\n",
    "            if truth:\n",
    "                newlist.append(0)\n",
    "            else:\n",
    "                newlist.append(item)\n",
    "    return newlist\n",
    "    \n",
    "#\n",
    "#\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "569eccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "#\n",
    "# Variable definitions\n",
    "\n",
    "# File architechture change because we can't nromalise multiple\n",
    "# blot files at the same time (Ab switches :/)\n",
    "#data_path = \"./excel_sheets/20241128_\" # and then the rest of the file name\n",
    "\n",
    "colour_dict = {\"JE6\" : \"grey\",\n",
    "               \"J.TCPTP-\" : \"deeppink\",\n",
    "               \"J.PTPN22-\" : \"deepskyblue\",\n",
    "               \"J.SHP1-\" : \"blueviolet\"}\n",
    "\n",
    "linestyle_dict = {\"0.05% DMSO\" : \"solid\",\n",
    "                  r\"20 $\\mu$M\" : \"dashdot\"}\n",
    "\n",
    "experiment = [700.0, '685Ex-720Em']\n",
    "control = [800.0, '785Ex-820Em']\n",
    "\n",
    "    # Titration variables\n",
    "\n",
    "#titr_files = [f\"{data_path}titration_4g10.xlsx\",\n",
    "#              f\"{data_path}titration_erk.xlsx\"]\n",
    "\n",
    "titr_groups = [\"JE6\", \"J.TCPTP-\", \"J.PTPN22-\", \"J.SHP1-\"]\n",
    "\n",
    "titr_xgroups = [\"0.05% DMSO 0m\", \"0.05% DMSO 5m\",\n",
    "                r\"0.2$\\mu$M 5m\", r\"2 $\\,mu$M 5m\",\n",
    "                r\"20 $\\mu$M 5m\", r\"50 $\\mu$M 5m\" ]\n",
    "\n",
    "    # Time course variables\n",
    "\n",
    "#time_files = [f\"{data_path}4g10.xlsx\",\n",
    "#              f\"{data_path}erk.xlsx\",\n",
    "#              f\"{data_path}plcg1.xlsx\",\n",
    "#              f\"{data_path}lck.xlsx\"]\n",
    "\n",
    "time_groups = [\"JE6 0.05% DMSO\", \n",
    "               \"J.TCPTP- 0.05% DMSO\",\n",
    "               \"J.PTPN22- 0.05% DMSO\", \n",
    "               \"J.SHP1- 0.05% DMSO\",\n",
    "               r\"JE6 20 $\\mu$M U0126\", \n",
    "               r\"J.TCPTP- 20 $\\mu$M U0126\",\n",
    "               r\"J.PTPN22- 20 $\\mu$M U0126\", \n",
    "               r\"J.SHP1- 20 $\\mu$M U0126\"]\n",
    "\n",
    "time_xgroups = [\"0 m\", \"2m\", \"5m\", \"10m\"]\n",
    "\n",
    "#\n",
    "#\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4666be86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "#\n",
    "#  Want to normalise two files individually, then merge with \n",
    "#  correct labels\n",
    "\n",
    "    # Assume we have files, and write from there\n",
    "    \n",
    "def find_correct_signal(a_file_df,\n",
    "                        signal_values,\n",
    "                        gs_kwargs):\n",
    "    \"\"\"\n",
    "    Goal: Try to grab the signal for all the given\n",
    "          signal values, then return the ones that worked\n",
    "          returns what values were found, if none then []\n",
    "    \"\"\"\n",
    "    found = []\n",
    "    for val in signal_values:\n",
    "        found = wh.get_signal(a_file_df,\n",
    "                              val,\n",
    "                              **gs_kwargs)\n",
    "        if len(found) > 0:\n",
    "            return found\n",
    "    return found\n",
    "    \n",
    "\n",
    "def get_all_signals(file_dfs,\n",
    "                    expr_signals = [700, \"685Ex-720Em\"],\n",
    "                    load_signals = [800, \"785Ex-820Em\"],\n",
    "                    gs_kwargs = dict(signal_column = \"Signal\",\n",
    "                                 channel_column = \"Channel\"),\n",
    "                    df_meta = [\"Group\", \"Condition\", \"Time\"]):\n",
    "    \"\"\"\n",
    "    Goal: Go through each file, grab the experimental/load\n",
    "          data, plus any specified metadata\n",
    "          return 3 lists: experimental signal, load_signal, \n",
    "                          metadata per row (only experimental)\n",
    "    \"\"\"\n",
    "    expr_out = []\n",
    "    load_out = []\n",
    "    metadata = []\n",
    "    for df in file_dfs:\n",
    "        # Temp holders for this file\n",
    "        expr_hold = find_correct_signal(df, expr_signals, gs_kwargs)\n",
    "        load_hold = find_correct_signal(df, load_signals, gs_kwargs)\n",
    "        try:\n",
    "            meta_hold = [find_correct_signal(df, expr_signals, {\"signal_column\" : metacheck,\n",
    "                                                                \"channel_column\" : \"Channel\"}) for metacheck in df_meta]\n",
    "        except:\n",
    "            meta_hold = []\n",
    "        # Add them to the returner lists\n",
    "        expr_out.append(expr_hold)\n",
    "        load_out.append(load_hold)\n",
    "        metadata.append(meta_hold)\n",
    "    # If metadata isn't empty,we want to reformat it\n",
    "    if metadata != []:\n",
    "        metadata = [gh.transpose(*g) for g in metadata]\n",
    "        metadata = [[gh.list_to_str(subg, \n",
    "                                    delimiter = \" \", \n",
    "                                    newline = False) for subg in g]\n",
    "                    for g in metadata]\n",
    "    return expr_out, load_out, metadata\n",
    "\n",
    "def read_norm_merge(files,\n",
    "                    expr_signal = [700, \"685Ex-720Em\"],\n",
    "                    load_signal = [800, \"785Ex-820Em\"],\n",
    "                    df_meta = [\"Group\", \"Condition\", \"Time\"],\n",
    "                    norm_string = \" 0m\",\n",
    "                    gs_kwargs = dict(signal_column = \"Signal\",\n",
    "                                 channel_column = \"Channel\"),\n",
    "                    log2_trans = True):\n",
    "    \"\"\"\n",
    "    Goal: Grab the dataframe files, use the Western Helpers\n",
    "          stuff to do some of the management/normalisation\n",
    "          and finally merge the files\n",
    "    \"\"\"\n",
    "    # First, get a list of all the files. We assume this whole\n",
    "    # directory is merging into one final file\n",
    "    files = glob.glob(f\"{files}/*\")\n",
    "    files = [pd.read_excel(f) for f in files]\n",
    "    \n",
    "    expr_sigs, load_sigs, metadata = get_all_signals(files,\n",
    "                                                     expr_signals = expr_signal,\n",
    "                                                     load_signals = load_signal,\n",
    "                                                     gs_kwargs = gs_kwargs,\n",
    "                                                     df_meta = df_meta)\n",
    "    # HOLY FUCK that took forever\n",
    "    # Alright, now that we have all the information we need, we\n",
    "    # can start croonchin noombres\n",
    "    corrected_sigs = [wh.licor_correction(expr_sigs[i],\n",
    "                                          load_sigs[i]) for i in range(len(expr_sigs))]\n",
    "    # With the corrected signal, we can normalise to the mean of the\n",
    "    # norm_group. We'll use the metadata strings to find the indices\n",
    "    # for that group\n",
    "    norm_inds = [[j for j in range(len(metadata[i])) if norm_string in metadata[i][j]]\n",
    "                 for i in range(len(metadata))]\n",
    "    norm_means = [[sh.mean([corrected_sigs[i][j] for j in norm_inds[i]],\n",
    "                          filter_nans = True,\n",
    "                          threshold = 1)]\n",
    "                  for i in range(len(corrected_sigs))]\n",
    "    norm_signals = [[corrected_sigs[i][j]/norm_means[i][0] for j in range(len(corrected_sigs[i]))]\n",
    "                    for i in range(len(corrected_sigs))]\n",
    "    if log2_trans:\n",
    "        norm_signals = [[log2(item) for item in group] for group in norm_signals]\n",
    "    # Now we need to get ready to merge...\n",
    "    # I'm thinking do the 'ol bullshit hack and use bin_by_col,\n",
    "    # then merge_dicts. So first add in headers and merge the metadata\n",
    "    d_heads = [\"Norm Signal\", \"Group Labels\"]\n",
    "    data = [[norm_signals[i], metadata[i]] for i in range(len(norm_signals))]\n",
    "    data = [gh.transpose(*g) for g in data]\n",
    "    data = [[d_heads] + g for g in data]\n",
    "    data = [gh.bin_by_col(g, d_heads.index(\"Group Labels\")) for g in data]\n",
    "    data = [{key : value[1:] for key, value in g.items()} for g in data]\n",
    "    dm = gh.merge_dicts(*data)\n",
    "    \n",
    "    # Merge dicts is weird, might not use it. but otherwise we just need\n",
    "    # to take the two sublists and actually merge them properly\n",
    "    return data, dm\n",
    "\n",
    "\n",
    "files = read_norm_merge(\"./excel_sheets/titration_4g10/je6\")\n",
    "\n",
    "#\n",
    "#\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "18b185c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'JE6 0.125% DMSO 0m': [[-0.22410053270984356, 'JE6 0.125% DMSO 0m'],\n",
       "   [0.19392483560871077, 'JE6 0.125% DMSO 0m']],\n",
       "  'JE6 0.125% DMSO 5m': [[2.7246874626494137, 'JE6 0.125% DMSO 5m'],\n",
       "   [2.6481184914628932, 'JE6 0.125% DMSO 5m']],\n",
       "  'JE6 0.2 $\\\\mu$M 5m': [[2.429423961825185, 'JE6 0.2 $\\\\mu$M 5m'],\n",
       "   [2.618218007990527, 'JE6 0.2 $\\\\mu$M 5m']],\n",
       "  'JE6 2 $\\\\mu$M 5m': [[2.716477534084825, 'JE6 2 $\\\\mu$M 5m'],\n",
       "   [2.9129835855129005, 'JE6 2 $\\\\mu$M 5m']],\n",
       "  'JE6 20 $\\\\mu$M 5m': [[0.9640608764821401, 'JE6 20 $\\\\mu$M 5m'],\n",
       "   [0.6251687896515562, 'JE6 20 $\\\\mu$M 5m']],\n",
       "  'JE6 50 $\\\\mu$M 5m': [[-0.10397072012047581, 'JE6 50 $\\\\mu$M 5m'],\n",
       "   [-0.15916000066599656, 'JE6 50 $\\\\mu$M 5m']]},\n",
       " {'JE6 0.125% DMSO 0m': [[-0.03901714736679002, 'JE6 0.125% DMSO 0m'],\n",
       "   [0.03798966998746572, 'JE6 0.125% DMSO 0m']],\n",
       "  'JE6 0.125% DMSO 5m': [[1.883291452956631, 'JE6 0.125% DMSO 5m'],\n",
       "   [2.659735706948073, 'JE6 0.125% DMSO 5m']],\n",
       "  'JE6 0.2 $\\\\mu$M 5m': [[1.8856366342092246, 'JE6 0.2 $\\\\mu$M 5m'],\n",
       "   [2.204422829871399, 'JE6 0.2 $\\\\mu$M 5m']],\n",
       "  'JE6 2 $\\\\mu$M 5m': [[2.118911298221995, 'JE6 2 $\\\\mu$M 5m'],\n",
       "   [1.8869339097974946, 'JE6 2 $\\\\mu$M 5m']],\n",
       "  'JE6 20 $\\\\mu$M 5m': [[0.08176472137054265, 'JE6 20 $\\\\mu$M 5m'],\n",
       "   [-2.921081905292272, 'JE6 20 $\\\\mu$M 5m']],\n",
       "  'JE6 50 $\\\\mu$M 5m': [[-1.3795960036754402, 'JE6 50 $\\\\mu$M 5m'],\n",
       "   [0.90646858310968, 'JE6 50 $\\\\mu$M 5m']]}]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2ccfbda2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JE6 0.125% DMSO 0m': [[[-0.22410053270984356, 'JE6 0.125% DMSO 0m'],\n",
       "   [0.19392483560871077, 'JE6 0.125% DMSO 0m']],\n",
       "  [[-0.03901714736679002, 'JE6 0.125% DMSO 0m'],\n",
       "   [0.03798966998746572, 'JE6 0.125% DMSO 0m']]],\n",
       " 'JE6 0.125% DMSO 5m': [[[2.7246874626494137, 'JE6 0.125% DMSO 5m'],\n",
       "   [2.6481184914628932, 'JE6 0.125% DMSO 5m']],\n",
       "  [[1.883291452956631, 'JE6 0.125% DMSO 5m'],\n",
       "   [2.659735706948073, 'JE6 0.125% DMSO 5m']]],\n",
       " 'JE6 0.2 $\\\\mu$M 5m': [[[2.429423961825185, 'JE6 0.2 $\\\\mu$M 5m'],\n",
       "   [2.618218007990527, 'JE6 0.2 $\\\\mu$M 5m']],\n",
       "  [[1.8856366342092246, 'JE6 0.2 $\\\\mu$M 5m'],\n",
       "   [2.204422829871399, 'JE6 0.2 $\\\\mu$M 5m']]],\n",
       " 'JE6 2 $\\\\mu$M 5m': [[[2.716477534084825, 'JE6 2 $\\\\mu$M 5m'],\n",
       "   [2.9129835855129005, 'JE6 2 $\\\\mu$M 5m']],\n",
       "  [[2.118911298221995, 'JE6 2 $\\\\mu$M 5m'],\n",
       "   [1.8869339097974946, 'JE6 2 $\\\\mu$M 5m']]],\n",
       " 'JE6 20 $\\\\mu$M 5m': [[[0.9640608764821401, 'JE6 20 $\\\\mu$M 5m'],\n",
       "   [0.6251687896515562, 'JE6 20 $\\\\mu$M 5m']],\n",
       "  [[0.08176472137054265, 'JE6 20 $\\\\mu$M 5m'],\n",
       "   [-2.921081905292272, 'JE6 20 $\\\\mu$M 5m']]],\n",
       " 'JE6 50 $\\\\mu$M 5m': [[[-0.10397072012047581, 'JE6 50 $\\\\mu$M 5m'],\n",
       "   [-0.15916000066599656, 'JE6 50 $\\\\mu$M 5m']],\n",
       "  [[-1.3795960036754402, 'JE6 50 $\\\\mu$M 5m'],\n",
       "   [0.90646858310968, 'JE6 50 $\\\\mu$M 5m']]]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "###################################################################\n",
    "#\n",
    "# Grab the Western data and start plotting\n",
    "\n",
    "# First, we're going to do the titration data because\n",
    "# it should be a bit simpler\n",
    "\n",
    "# Read the excel file using Pandas, and send it to a list\n",
    "titres = [pd.read_excel(f) for f in titr_files]\n",
    "\n",
    "### Need to normalise here\n",
    "\n",
    "# Save the column headers\n",
    "titre_cols = list(titres[0].columns)\n",
    "titres = [[list(row) for row in df.to_numpy()] for df in titres]\n",
    "titres = [[titre_cols] + file for file in titres]\n",
    "# Fix the numbers\n",
    "titres = [[gh.transform_values(row) for row in file] for file in titres]\n",
    "titres = [[neg_to_zero(row) for row in file] for file in titres]\n",
    "# Now we want to try and split the matrices on the group column\n",
    "titres = [gh.bin_by_col(file, titre_cols.index(\"Group\"),\n",
    "                        head_row = 0) for file in titres]\n",
    "\n",
    "# So now, we have the groups binned, and we need to grab the groups\n",
    "# and label them. I think I can do this with bin_by_col\n",
    "# again on condition and time, then the label strings are\n",
    "#    <group key> <condition key> <time key>\n",
    "\n",
    "# List of dict[group] = list(values)\n",
    "titres = [{key : gh.bin_by_col(value, \n",
    "                               titre_cols.index(\"Condition\"),\n",
    "                               head_row = 0) for key, value in file.items()} for file in titres]\n",
    "\n",
    "# List of dict[group] = dict[condition] = list(values)\n",
    "titres = [{gkey : {ckey : gh.bin_by_col(cval,\n",
    "                                        titre_cols.index(\"Time\"),\n",
    "                                        head_row = 0) for ckey, cval in gval.items()} for gkey, gval in file.items()\n",
    "          } for file in titres]\n",
    "print(titres[0][\"JE6\"][\"0.125% DMSO\"][\"0m\"])\n",
    "# Now that everything is binned, we need to normalise to the 800\n",
    "# channel. All lists should be \n",
    "# [heads]\n",
    "# [700, r1-r4]\n",
    "# [800, r1-r4]\n",
    "titres = [{gkey : {ckey : {tkey : [row[titre_cols.index(\"Signal\")] for row in tval[1:]]\n",
    "                           for tkey, tval in cval.items()}\n",
    "                  for ckey, cval in gval.items()}\n",
    "           for gkey, gval in file.items()}\n",
    "          for file in titres]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
