{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The phosphatases TCPTP, PTPN22, and SHP1 play unique roles in T cell phosphotyrosine maintenance and feedback regulation of the TCR\n",
    "\n",
    "\n",
    "<b>Authors:</b>\n",
    "\n",
    "Aurora Callahan1, Aisharja Mojumdar2, Mengzhou Hu2, Amber Wang1, Alijah A Griffith1, Nicholas Huang1, Xien Yu Chua2, Nick Mroz1, Ryan Z. Puterbaugh2, Shanelle Reilly1, Arthur Salomon1,2,*\n",
    "\n",
    "<b>Affiliations:</b>\n",
    "\n",
    "1 Department of Molecular Biology, Cell Biology and Biochemistry, Brown University, Providence, RI, 02903, United States of America\n",
    "\n",
    "2 Department of Molecular Pharmacology, Physiology, and Biotechnology, Brown University, Providence, RI, 02903, United States of America\n",
    "\n",
    ".* Corresponding Author, art@drsalomon.com\n",
    "\n",
    "\n",
    "<b>Abstract</b>\n",
    "\n",
    "The protein tyrosine phosphatases (PTPs) TCPTP, PTPN22, and SHP1 are critical regulators of the activating phophotyrosine (pY) site on the initiating T cell kinase, LckY394, but the broader implications of these phosphatases in T cell receptor (TCR) signalling and T cell biology remain unclear. By combining CRISPR/Cas9 gene editing and mass spectrometry, we evaluate the protein- and pY-level effects of TCPTP, PTPN22, and SHP1 in the Jurkat T cell model system. We find that deletion of each phosphatase corresponds to unique changes in the proteome of T cells with few large-scale changes to TCR signalling proteins. Notably, PTPN22 and SHP1 deletions have opposing effects on pY abundance globally, while TCPTP deletion modestly elevates pY levels. Finally, we show that TCPTP is indirectly involved in Erk1/2 positive feedback to the TCR. Overall, our work provides evidence for alternative functions of three T cell phosphatases long thought to be redundant.\n",
    "\n",
    "<b>Notebook Synopsis</b>\n",
    "\n",
    "This notebook takes the Excel output from the high throughput autonomous proteomics pipeline (HTAPP)<a href=\"https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/pmic.200900159?casa_token=ZI5Ml2QqE7sAAAAA%3AxXrc3KZbzG1iyRIi3afNoHXGJA6cc0Ot9fYuiYPUOJP88zd2XJPYBsjiL7b8Oezh1GJUI4dKZpYvgA\">[1]</a> and PeptideDepot<a href=\"https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/pmic.200900119?casa_token=BzpTzA0oeHYAAAAA%3AcHtUnF72XzUz4DslIx6Eq0BpMuDxryHPR43APtNnittn1Ry-rD5UqsxAWo1Bg_PnBX4KB4WHtRODiA\">[2]</a> developed by the Salomon Laboratory and produces the graphs used in the above manuscript. Notes regarding the steps will be shown above each code cell.\n",
    "\n",
    "The first cell imports the necessary dependencies to perform data analysis, including Aurora's 'helpers' package, which can be found as a zip file in <a href=\"https://github.com/Aurdeegz/CAR-Target-Signalling-code/blob/master/proteomics_data_analysis/analysis_pipeline.ipynb\">this repository</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 3.12.9\n"
     ]
    }
   ],
   "source": [
    "## Importables\n",
    "from platform import python_version\n",
    "print(f\"Python version {python_version()}\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "\n",
    "sys.path.append(\"/windir/c/Users/redas/Desktop/jupyter_directory/helpers/src/helpers/\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from math import log10, log2, ceil, floor, sqrt\n",
    "from missforest import MissForest\n",
    "\n",
    "# And grab the helpers\n",
    "import sys\n",
    "sys.path.append(\"/windir/c/Users/redas/Desktop/jupyter_directory/helpers/src/helpers/\")\n",
    "from helpers import general_helpers as gh\n",
    "from helpers import stats_helpers as sh\n",
    "from helpers import mpl_plotting_helpers as mph\n",
    "from helpers.proteomics_helpers import Peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables\n",
    "filename = \"PTPN22_TCPTP_SHP1_U0126_pTyr_DDA_2.xls\"\n",
    "\n",
    "rename_cols = {\"UNIPROT Gene Name\" : \"Gene name\",                                \n",
    "               \"phosphosite annotated\" : \"Site\",                            \n",
    "                \"peakarea manual 1 rep1 thresholded timepoint1\" : \"JE6 0m DMSO R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint1\" : \"JE6 0m DMSO R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint1\" : \"JE6 0m DMSO R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint1\" : \"JE6 0m DMSO R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint1\" : \"JE6 0m DMSO R5\",\n",
    "                \"peakarea manual 1 rep1 thresholded timepoint2\" : \"JE6 5m DMSO R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint2\" : \"JE6 5m DMSO R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint2\" : \"JE6 5m DMSO R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint2\" : \"JE6 5m DMSO R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint2\" : \"JE6 5m DMSO R5\", \n",
    "                \"peakarea manual 1 rep1 thresholded timepoint3\" : \"JE6 0m U0126 R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint3\" : \"JE6 0m U0126 R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint3\" : \"JE6 0m U0126 R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint3\" : \"JE6 0m U0126 R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint3\" : \"JE6 0m U0126 R5\",\n",
    "                \"peakarea manual 1 rep1 thresholded timepoint4\" : \"JE6 5m U0126 R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint4\" : \"JE6 5m U0126 R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint4\" : \"JE6 5m U0126 R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint4\" : \"JE6 5m U0126 R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint4\" : \"JE6 5m U0126 R5\", \n",
    "               \"peakarea manual 1 rep1 thresholded timepoint9\" : \"J.TCPTP- 0m DMSO R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint9\" : \"J.TCPTP- 0m DMSO R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint9\" : \"J.TCPTP- 0m DMSO R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint9\" : \"J.TCPTP- 0m DMSO R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint9\" : \"J.TCPTP- 0m DMSO R5\",\n",
    "                \"peakarea manual 1 rep1 thresholded timepoint10\" : \"J.TCPTP- 5m DMSO R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint10\" : \"J.TCPTP- 5m DMSO R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint10\" : \"J.TCPTP- 5m DMSO R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint10\" : \"J.TCPTP- 5m DMSO R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint10\" : \"J.TCPTP- 5m DMSO R5\", \n",
    "                \"peakarea manual 1 rep1 thresholded timepoint11\" : \"J.TCPTP- 0m U0126 R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint11\" : \"J.TCPTP- 0m U0126 R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint11\" : \"J.TCPTP- 0m U0126 R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint11\" : \"J.TCPTP- 0m U0126 R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint11\" : \"J.TCPTP- 0m U0126 R5\",\n",
    "                \"peakarea manual 1 rep1 thresholded timepoint12\" : \"J.TCPTP- 5m U0126 R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint12\" : \"J.TCPTP- 5m U0126 R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint12\" : \"J.TCPTP- 5m U0126 R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint12\" : \"J.TCPTP- 5m U0126 R4\",\n",
    "#                \"peakarea manual 1 rep5 thresholded timepoint12\" : \"J.TCPTP- 5m U0126 R5\", Replicate has no sequencing\n",
    "                \"peakarea manual 1 rep1 thresholded timepoint5\" : \"J.PTPN22- 0m DMSO R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint5\" : \"J.PTPN22- 0m DMSO R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint5\" : \"J.PTPN22- 0m DMSO R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint5\" : \"J.PTPN22- 0m DMSO R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint5\" : \"J.PTPN22- 0m DMSO R5\",\n",
    "                \"peakarea manual 1 rep1 thresholded timepoint6\" : \"J.PTPN22- 5m DMSO R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint6\" : \"J.PTPN22- 5m DMSO R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint6\" : \"J.PTPN22- 5m DMSO R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint6\" : \"J.PTPN22- 5m DMSO R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint6\" : \"J.PTPN22- 5m DMSO R5\", \n",
    "                \"peakarea manual 1 rep1 thresholded timepoint7\" : \"J.PTPN22- 0m U0126 R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint7\" : \"J.PTPN22- 0m U0126 R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint7\" : \"J.PTPN22- 0m U0126 R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint7\" : \"J.PTPN22- 0m U0126 R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint7\" : \"J.PTPN22- 0m U0126 R5\",\n",
    "                \"peakarea manual 1 rep1 thresholded timepoint8\" : \"J.PTPN22- 5m U0126 R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint8\" : \"J.PTPN22- 5m U0126 R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint8\" : \"J.PTPN22- 5m U0126 R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint8\" : \"J.PTPN22- 5m U0126 R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint8\" : \"J.PTPN22- 5m U0126 R5\",\n",
    "                \"peakarea manual 1 rep1 thresholded timepoint13\" : \"J.SHP1- 0m DMSO R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint13\" : \"J.SHP1- 0m DMSO R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint13\" : \"J.SHP1- 0m DMSO R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint13\" : \"J.SHP1- 0m DMSO R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint13\" : \"J.SHP1- 0m DMSO R5\",\n",
    "                \"peakarea manual 1 rep1 thresholded timepoint14\" : \"J.SHP1- 5m DMSO R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint14\" : \"J.SHP1- 5m DMSO R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint14\" : \"J.SHP1- 5m DMSO R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint14\" : \"J.SHP1- 5m DMSO R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint14\" : \"J.SHP1- 5m DMSO R5\", \n",
    "                \"peakarea manual 1 rep1 thresholded timepoint15\" : \"J.SHP1- 0m U0126 R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint15\" : \"J.SHP1- 0m U0126 R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint15\" : \"J.SHP1- 0m U0126 R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint15\" : \"J.SHP1- 0m U0126 R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint15\" : \"J.SHP1- 0m U0126 R5\",\n",
    "                \"peakarea manual 1 rep1 thresholded timepoint16\" : \"J.SHP1- 5m U0126 R1\",   \n",
    "                \"peakarea manual 1 rep2 thresholded timepoint16\" : \"J.SHP1- 5m U0126 R2\",   \n",
    "                \"peakarea manual 1 rep3 thresholded timepoint16\" : \"J.SHP1- 5m U0126 R3\",   \n",
    "                \"peakarea manual 1 rep4 thresholded timepoint16\" : \"J.SHP1- 5m U0126 R4\",\n",
    "                \"peakarea manual 1 rep5 thresholded timepoint16\" : \"J.SHP1- 5m U0126 R5\",        \n",
    "                \"assigned sequence\" : \"Modified Sequence\", \n",
    "                \"Kegg unique index\" : \"KEGG category\",\n",
    "                \"peptide sequence GCT format centered on 1st site\" : \"Flanking Sequence (1)\",\n",
    "                \"peptide sequence GCT format centered on 2nd site\" : \"Flanking Sequence (2)\",\n",
    "                \"peptide sequence GCT format centered on 3rd site\" : \"Flanking Sequence (3)\"\n",
    "              }\n",
    "\n",
    "colours = [mph.colours[\"monos\"][3:7],\n",
    "           mph.colours[\"pinks\"][2:6],\n",
    "           mph.colours[\"blues\"][-4:],\n",
    "           mph.colours[\"purples\"][-4:]]\n",
    "\n",
    "\n",
    "group_inds = {\"JE6 0m DMSO\" : [2,3,4,5,6],\n",
    "              \"JE6 5m DMSO\" : [7,8,9,10,11],\n",
    "              \"JE6 0m U0126\" : [12,13,14,15,16],\n",
    "              \"JE6 5m U0126\" : [17,18,19,20,21],\n",
    "              \"TCP 0m DMSO\"  : [22,23,24,25,26],\n",
    "              \"TCP 5m DMSO\"  : [27,28,29,30,31],\n",
    "              \"TCP 0m U0126\" : [32,33,34,35,36],\n",
    "              \"TCP 5m U0126\" : [37,38,39,40],\n",
    "              \"N22 0m DMSO\"  : [41,42,43,44,45],\n",
    "              \"N22 5m DMSO\"  : [46,47,48,49,50],\n",
    "              \"N22 0m U0126\" : [51,52,53,54,55],\n",
    "              \"N22 5m U0126\" : [56,57,58,59,60],\n",
    "              \"SHP 0m DMSO\"  : [61,62,63,64,65],\n",
    "              \"SHP 5m DMSO\"  : [66,67,68,69,70],\n",
    "              \"SHP 0m U0126\" : [71,72,73,74,75],\n",
    "              \"SHP 5m U0126\" : [76,77,78,79,80]\n",
    "             }\n",
    "\n",
    "comparisons = [(\"TCP 0m DMSO\", \"JE6 0m DMSO\"),\n",
    "               (\"N22 0m DMSO\", \"JE6 0m DMSO\"),\n",
    "               (\"SHP 0m DMSO\", \"JE6 0m DMSO\"), # Basal state/JE6 comps DMSO\n",
    "               (\"TCP 5m DMSO\", \"JE6 5m DMSO\"),\n",
    "               (\"N22 5m DMSO\", \"JE6 5m DMSO\"),\n",
    "               (\"SHP 5m DMSO\", \"JE6 5m DMSO\"), # Stim state/JE6 comps DMSO\n",
    "               (\"JE6 5m DMSO\", \"JE6 0m DMSO\"),\n",
    "               (\"TCP 5m DMSO\", \"TCP 0m DMSO\"),\n",
    "               (\"N22 5m DMSO\", \"N22 0m DMSO\"),\n",
    "               (\"SHP 5m DMSO\", \"SHP 0m DMSO\"), # 5m Stims DMSO\n",
    "               (\"TCP 0m U0126\", \"JE6 0m U0126\"),\n",
    "               (\"N22 0m U0126\", \"JE6 0m U0126\"),\n",
    "               (\"SHP 0m U0126\", \"JE6 0m U0126\"), # Basal state/JE6 comps DMSO\n",
    "               (\"TCP 5m U0126\", \"JE6 5m U0126\"),\n",
    "               (\"N22 5m U0126\", \"JE6 5m U0126\"),\n",
    "               (\"SHP 5m U0126\", \"JE6 5m U0126\"), # Stim state/JE6 comps DMSO\n",
    "               (\"JE6 0m U0126\", \"JE6 0m DMSO\"),\n",
    "               (\"TCP 0m U0126\", \"TCP 0m DMSO\"),\n",
    "               (\"N22 0m U0126\", \"N22 0m DMSO\"),\n",
    "               (\"SHP 0m U0126\", \"SHP 0m DMSO\"), # Basal state +/- U0126\n",
    "               (\"JE6 5m U0126\", \"JE6 5m DMSO\"),\n",
    "               (\"TCP 5m U0126\", \"TCP 5m DMSO\"),\n",
    "               (\"N22 5m U0126\", \"N22 5m DMSO\"),\n",
    "               (\"SHP 5m U0126\", \"SHP 5m DMSO\"), # Stim state +/- U0126\n",
    "               (\"JE6 5m U0126\", \"JE6 0m DMSO\"),\n",
    "               (\"TCP 5m U0126\", \"TCP 0m DMSO\"),\n",
    "               (\"N22 5m U0126\", \"N22 0m DMSO\"),\n",
    "               (\"SHP 5m U0126\", \"SHP 0m DMSO\"), # 5m stims U0126\n",
    "               ]\n",
    "\n",
    "reps_2 = [3,8,13,18,23,28,33,38,42,47,52,57,62,67,72,77]\n",
    "\n",
    "textdict = dict(fontfamily = \"sans-serif\",\n",
    "                font = \"Arial\",\n",
    "                fontsize = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the module: helpers.general_helpers\n",
      "\n",
      "Loading the module: helpers.argcheck_helpers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the things for ssgsea\n",
    "from py_scripts import generate_ssgsea_files as gsf\n",
    "from py_scripts import managing_ssgsea_outputs as mso\n",
    "\n",
    "# For GCT file writing\n",
    "## Need to keep only the input values and flanks\n",
    "\n",
    "def ptmsea_transform(a_qvalue, a_foldchange, head = \"No\"):\n",
    "    if type(a_qvalue) == str or type(a_foldchange) == str:\n",
    "        return head\n",
    "    if a_qvalue != a_qvalue or a_foldchange != a_foldchange:\n",
    "        return float(\"nan\")\n",
    "    if a_foldchange < 0:\n",
    "        return -10 * log10(a_qvalue) *-1\n",
    "    else:\n",
    "        return -10 * log10(a_qvalue) * 1\n",
    "\n",
    "def duplicate_flanks(a_row,\n",
    "                     flank_locs,   # list of indices for the places with flanking sequences\n",
    "                     ):\n",
    "    flank_num = len(flank_locs)\n",
    "    new_rows = [[] for _ in range(flank_num)] # Make the new rows\n",
    "    flanks = []\n",
    "    f_ind = -1\n",
    "    seen = False\n",
    "    for i in range(len(a_row)):               # loop over the size of the row\n",
    "        if i not in flank_locs:               # If this is not a flank\n",
    "            for j in range(flank_num):\n",
    "                new_rows[j].append(a_row[i])  # Add this info to each sublist\n",
    "        elif i in flank_locs:\n",
    "            flanks.append(a_row[i])\n",
    "            if not seen:\n",
    "                for j in range(flank_num):\n",
    "                    new_rows[j].append(\"FLANK ME BABY\")\n",
    "                seen = True\n",
    "            if f_ind == -1:\n",
    "                f_ind = i\n",
    "    for i in range(flank_num):\n",
    "        new_rows[i][f_ind] = flanks[i]\n",
    "    return new_rows\n",
    "\n",
    "def dup_flanks_matrix(a_matrix, flank_locs):\n",
    "    new_matr = []\n",
    "    for row in a_matrix:\n",
    "        exp = duplicate_flanks(row, flank_locs)\n",
    "        new_matr += exp\n",
    "    return new_matr\n",
    "\n",
    "def write_gct_file(index,    # Flanking sequences with -p \n",
    "                    values,   # values per flanking sequence, len(values[i]) == len(headers), len(values) == len(index)\n",
    "                    headers, # Headers for the columns\n",
    "                    outfile = \"bullshit.gct\" # File to write, include path and .gct\n",
    "                    ):\n",
    "    # GCT file has some info up top, we take it\n",
    "    gct = [[fr\"#1.3\"], # Tells programs its a GCT 1.3 file\n",
    "           [len(index), len(headers), 0, 0], # number of rows, number of cols, and metadata shit\n",
    "           [\"flanking_seq\"] + headers] \n",
    "    for i in range(len(index)):\n",
    "        gct.append([index[i]] + values[i])\n",
    "    gct = [gh.list_to_str(row) for row in gct]\n",
    "    gh.write_outfile(gct, outfile, writestyle = \"w\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_first(a_matrix, keycol = \"U_ID\", heads = 0):\n",
    "    keepers = []\n",
    "    seen = []\n",
    "    keyind = a_matrix[heads].index(keycol)\n",
    "    for row in a_matrix:\n",
    "        if row[keyind] not in seen:\n",
    "            keepers.append(row)\n",
    "            seen.append(row[keyind])\n",
    "    return keepers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading files in and massaging the data\n",
    "\n",
    "\n",
    "def filter_df_rows(a_df, column = \"U_ID\"):\n",
    "    cols = list(a_df.columns)\n",
    "    keycol = cols.index(column)\n",
    "    a_list = list(a_df.to_numpy())\n",
    "    first_occ = []\n",
    "    first_key = []\n",
    "    for row in a_list:\n",
    "        if row[keycol] not in first_key:\n",
    "            first_occ.append(row)\n",
    "            first_key.append(row[keycol])\n",
    "        else:\n",
    "            pass\n",
    "    return pd.DataFrame(first_occ, columns = cols)\n",
    "\n",
    "def read_and_filter(a_file, columns, rename = False):\n",
    "    file = pd.read_excel(a_file)\n",
    "    file = file[columns]\n",
    "    if rename:\n",
    "        file = file.rename(rename, axis = \"columns\")\n",
    "    file[\"U_ID\"] = file[\"Gene name\"].map(str) + \"$^{\" + file[\"Site\"].map(str) + \"}$\"\n",
    "    file[\"Median Row Intensity\"] = file[[col for col in list(file.columns) if \" R\" in col]].median(axis=1)\n",
    "    file[\"Total Missing Values in Row\"] = file[[col for col in list(file.columns) if \" R\" in col]].isnull().sum(axis=1)\n",
    "    f = sorted([list(row) for row in file.to_numpy()], key = lambda x: (x[-3], x[-1], -x[-2]))\n",
    "    print(len(f))\n",
    "    f = keep_first([list(file.columns)] + f, keycol = \"U_ID\", heads = 0)\n",
    "    print(len(f))\n",
    "    file = pd.DataFrame(f[1:], columns = f[0])\n",
    "    return file\n",
    "\n",
    "def find_lims(all_qs,all_fcs):\n",
    "    qs = gh.unpack_list(all_qs)\n",
    "    fc = [abs(item) for item in gh.unpack_list(all_fcs) if item == item]\n",
    "    return round(-log10(min(qs)))+1, round(max(fc))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA function to make my life less miserable\n",
    "def square_bounds(mpl_axes, inplace = False):\n",
    "    ticks = list(mpl_axes.get_xticks()) + list(mpl_axes.get_yticks())\n",
    "    if inplace:\n",
    "        mpl_axes.set_xlim(min(ticks), max(ticks))\n",
    "        mpl_axes.set_ylim(min(ticks), max(ticks))\n",
    "    else:\n",
    "        return min(ticks), max(ticks)\n",
    "\n",
    "def pca_analysis(a_df, pca_kwargs = dict(n_components = 2,\n",
    "                                         whiten = False,\n",
    "                                         svd_solver = \"full\",\n",
    "                                         tol = 0)):\n",
    "    std_scalar = StandardScaler()\n",
    "    scaled = std_scalar.fit_transform(a_df.transpose())\n",
    "    pca_analysis = PCA(**pca_kwargs)\n",
    "    components = pca_analysis.fit_transform(scaled)\n",
    "    components = gh.transpose(*[list(point) for point in list(components)])\n",
    "    return components, pca_analysis\n",
    "\n",
    "def nmf_analysis(a_df, nmf_kwargs = dict(n_components = 2, \n",
    "                                     init = \"nndsvd\", # preserves sparseness\n",
    "                                     solver = \"mu\", # multiplicative update\n",
    "                                     beta_loss = \"frobenius\", # stable, but slow\n",
    "                                     alpha_W = 0,  # default\n",
    "                                     alpha_H = 0,  # default\n",
    "                                     l1_ratio = 0  # default\n",
    "                                    )):\n",
    "    #std_scalar = StandardScaler()\n",
    "    #scaled = std_scalar.fit_transform(a_df.transpose())\n",
    "    norms = normalize(a_df)\n",
    "    nmf_analysis = NMF(**nmf_kwargs)\n",
    "    W = nmf_analysis.fit_transform(norms)\n",
    "    H = nmf_analysis.components_\n",
    "    return H, W\n",
    "    \n",
    "def cluster_plotting(dataframes, # list with minimum 1 df\n",
    "                 groups,     \n",
    "                 expnames, # should match len(dataframes)\n",
    "                 filenames,# should match len(dataframes)\n",
    "                 group_slices, #assumes reps are clustered in list\n",
    "                 labels,       # should correspons to len(group_slices)\n",
    "                 colours,      # list of lists, each sublist should correspond to len(group_slices)\n",
    "                 markers = [\"o\",\"^\", \"s\"],\n",
    "                 cluster = 'PCA', # other option is NNMF\n",
    "                 markersize=100, \n",
    "                 textdict = dict(fontfamily = \"sans-serif\",\n",
    "                 font = \"Arial\",\n",
    "                 fontweight = \"bold\",\n",
    "                 fontsize = 10),\n",
    "                 square = True,\n",
    "                 pca_kwargs = dict(n_components = 30,\n",
    "                                   whiten = False,\n",
    "                                   svd_solver = \"full\",\n",
    "                                   tol = 0),\n",
    "                 nmf_kwargs = dict(n_components = 2, \n",
    "                                   init = \"nndsvd\", # preserves sparseness\n",
    "                                   solver = \"mu\", # multiplicative update\n",
    "                                   beta_loss = \"frobenius\", # stable, but slow\n",
    "                                   alpha_W = 0,  # default\n",
    "                                   alpha_H = 0,  # default\n",
    "                                   l1_ratio = 0  # default\n",
    "                                    )):\n",
    "    # Get the data columns from the dataframes, remove\n",
    "    # missing values, run PCA analysis with sklearn,\n",
    "    # scatter\n",
    "    \n",
    "    # Grab the columns corresponding to the groups of data. Assumes the 'groups' strings\n",
    "    # are a substring of the column headers\n",
    "    dfs = [df[[name for name in list(df.columns) if any(s in name for s in groups)]] for df in dataframes]\n",
    "    # Remove any row with missing values, as PCA doesn't tolerate MVs\n",
    "    dfs = [df.dropna() for df in dfs]\n",
    "    axes = []\n",
    "    i = 0\n",
    "    for df in dfs:\n",
    "        if cluster.lower() == \"pca\":\n",
    "            components,pca = pca_analysis(df, pca_kwargs = pca_kwargs)\n",
    "        else:\n",
    "            # will add nmf soon\n",
    "            components,nmf = nmf_analysis(df, nmf_kwargs = nmf_kwargs)\n",
    "        fig, ax = plt.subplots(figsize = (6,6))\n",
    "        # Next, loop over the slices and scatter\n",
    "        j = 0\n",
    "        for g in group_slices:\n",
    "            ax.scatter(components[0][g], components[1][g], \n",
    "                       color = colours[i][j],\n",
    "                       marker = markers[j], \n",
    "                       s = markersize, \n",
    "                       alpha = 0.75,          # my preference\n",
    "                       label = labels[j],\n",
    "                       edgecolor = \"black\",   # my preference\n",
    "                      )\n",
    "            j+=1\n",
    "        ax.set_title(expnames[i], **textdict)\n",
    "        if cluster.lower() == \"pca\":\n",
    "            ax.set_xlabel(f\"PC1 ({100*pca.explained_variance_ratio_[0]:.2f}%)\",**textdict)\n",
    "            ax.set_ylabel(f\"PC2 ({100*pca.explained_variance_ratio_[1]:.2f}%)\", **textdict)\n",
    "            #square_bounds(ax, inplace = True)\n",
    "        else:\n",
    "            # will add nmf soon\n",
    "            ax.set_xlabel(f\"Component 1\", **textdict)\n",
    "            ax.set_ylabel(f\"Component 2\", **textdict)\n",
    "        if square:\n",
    "            square_bounds(ax, inplace = True)\n",
    "        mph.update_ticks(ax, which = \"x\")\n",
    "        mph.update_ticks(ax, which =\"y\")\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filenames[i])\n",
    "        axes.append(ax)\n",
    "        plt.close()\n",
    "        i+=1\n",
    "    return axes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple Regression, because fuck doing the pairwise bullshit\n",
    "\n",
    "def format_linreg_strs(coeffs, r2, intercept, label = \"Biggest Dong\"):\n",
    "    outstr = f\"{label}\\n$y={intercept:.3f}\"\n",
    "    for i in range(len(coeffs)):\n",
    "        outstr += fr\"+{coeffs[i]:.3f}x_{{{i+1}}}\"\n",
    "    outstr += f\"$\\n$R={r2:.3f}$\"\n",
    "    return outstr\n",
    "\n",
    "def plot_linreg_strs(strs, save = \"test.pdf\",\n",
    "                    fontdict = dict(fontfamily = \"sans-serif\",\n",
    "                                      font = \"Arial\",\n",
    "                                      fontweight = \"bold\",\n",
    "                                      fontsize = 2)):\n",
    "    \"\"\"\n",
    "    Just plot the strings to exploit LaTeX math formatting\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    # Check how many strings there are, and adjust the axes accordingly\n",
    "    num = len(strs)\n",
    "    ax.set_xlim(-1,1)\n",
    "    ax.set_ylim(-1,num)\n",
    "    ax.set_yticks(list(range(num)))\n",
    "    # turn off the bounding box\n",
    "    ax.axis(\"off\")\n",
    "    # plot the strings\n",
    "    for i in range(num):\n",
    "        ax.text(0, i, strs[i], ha = \"center\", va = \"center\", **fontdict)\n",
    "    plt.savefig(save)\n",
    "    plt.close()\n",
    "    \n",
    "def multi_reg_lineplot(file, #dataframe\n",
    "                       groups = [\"t1\", \"t2\", \"t3\"], #substring of header, group indicator\n",
    "                       labels = [\"0 min\", \"2 min\", \"5 min\"], # Goes above the strings\n",
    "                       log2_trans = True,\n",
    "                       savefile = \"test.pdf\", # path/to/file.pdf\n",
    "                       fontdict= dict(fontfamily = \"sans-serif\",\n",
    "                                      font = \"Arial\",\n",
    "                                      fontweight = \"bold\",\n",
    "                                      fontsize = 2)\n",
    "                       ):\n",
    "    g_num = len(groups)\n",
    "    split_f = [file[[c for c in list(file.columns) if groups[i] in c]] for i in range(g_num)]\n",
    "    if log2_trans:\n",
    "        split_f = [[list(row) for row in list(g.astype(float).transform(np.log2).to_numpy())] for g in split_f]\n",
    "    else:\n",
    "        split_f = [[list(row) for row in list(g.astype(float).to_numpy())] for g in split_f]\n",
    "    # LinearRegression can't take missing values\n",
    "    split_f = [[row for row in g if all([item == item for item in row])] for g in split_f]\n",
    "    xs = [[row[:-1] for row in g] for g in split_f]\n",
    "    ys = [gh.transpose(*[[row[-1]] for row in g])[0] for g in split_f]\n",
    "    # Set up the model\n",
    "    linmods = [LinearRegression() for _ in range(g_num)]\n",
    "    # and fit it, always assume y is the last replicate in a group\n",
    "    regs = [linmods[i].fit(xs[i], ys[i]) for i in range(g_num)]\n",
    "    scores = [sqrt(regs[i].score(xs[i],ys[i])) for i in range(g_num)]\n",
    "    # Now we make the strings\n",
    "    strs = [format_linreg_strs(regs[i].coef_, scores[i], regs[i].intercept_, labels[i]) for i in range(g_num)]\n",
    "    # And pass them to the plotter\n",
    "    plot_linreg_strs(strs, save=savefile, fontdict = fontdict)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1222\n",
      "798\n"
     ]
    }
   ],
   "source": [
    "# Filters and removes redundancy, fewest MVs and highest median intensity\n",
    "file = read_and_filter(filename, list(rename_cols.keys()), rename_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA plots & correlation, pre imputation\n",
    "\n",
    "def uniq_list(a_list):\n",
    "    seen = []\n",
    "    for item in a_list:\n",
    "        if item not in seen:\n",
    "            seen.append(item)\n",
    "    return seen\n",
    "\n",
    "sample_groups = [gh.list_to_str(c.split(\" \")[:3], \n",
    "                                delimiter = \" \",\n",
    "                                newline = False) for c in list(rename_cols.values()) if \" R\" in c]\n",
    "\n",
    "sample_groups = uniq_list(sample_groups)\n",
    "\n",
    "colnames = [c for c in list(rename_cols.values()) if \" R\" in c]\n",
    "abundances = [list(row) for row in file[colnames].astype(float).to_numpy()]\n",
    "\n",
    "sample_cols = gh.transpose(*abundances)\n",
    "sample_cols = [col for col in sample_cols if not all([item != item for item in col])]\n",
    "sample_cols = gh.transpose(*sample_cols)\n",
    "sample_cols = [row for row in sample_cols if not any([item != item for item in row])]\n",
    "sample_cols = pd.DataFrame(sample_cols, columns = [c for c in list(rename_cols.values()) if \" R\" in c])\n",
    "sample_cols = sample_cols.apply(np.log2)\n",
    "\n",
    "pca_out_d_preimp = \"figs/qc/preimp/dmso_pca.pdf\"\n",
    "nmf_out_d_preimp = \"figs/qc/preimp/dmso_nmf.pdf\"\n",
    "\n",
    "pca_out_u_preimp = \"figs/qc/preimp/u0126_pca.pdf\"\n",
    "nmf_out_u_preimp = \"figs/qc/preimp/u0126_nmf.pdf\"\n",
    "\n",
    "\n",
    "\n",
    "multi_reg_lineplot(sample_cols, groups = sample_groups,\n",
    "                   labels = sample_groups, log2_trans = False,\n",
    "                   savefile = \"figs/qc/preimp/multi_linreg.pdf\")\n",
    "\n",
    "#\"\"\"\n",
    "nmf_ax = cluster_plotting([sample_cols[[c for c in list(rename_cols.values()) if \"DMSO\" in c]]], # list with minimum 1 df\n",
    "                 [s for s in sample_groups if \"DMSO\" in s],         \n",
    "                 [fr\"NMF Clustering 0.04% DMSO-treated pY samples\"],\n",
    "                 [nmf_out_d_preimp],\n",
    "                 [slice(5*i,5*(i+1)) for i in range(8)],\n",
    "                 [\"JE6 0m DMSO\", \"JE6 5m DMSO\",\n",
    "                  \"J.TCPTP- 0m DMSO\", \"J.TCPTP- 5m DMSO\",\n",
    "                  \"J.PTPN22- 0m DMSO\", \"J.PTPN22- 5m DMSO\",\n",
    "                  \"J.SHP1- 0m DMSO\", \"J.SHP1- 5m DMSO\"],\n",
    "                 [gh.unpack_list([c[:2] for c in colours])],\n",
    "                 markers = [\"o\",\"^\",\"o\",\"^\",\"o\",\"^\",\"o\",\"^\"],\n",
    "                 cluster = 'NMF',\n",
    "                 markersize=100,\n",
    "                 textdict = dict(fontfamily = \"sans-serif\",\n",
    "                 font = \"Arial\",\n",
    "                 fontweight = \"bold\",\n",
    "                 fontsize = 10),\n",
    "                          square = False,\n",
    "                 nmf_kwargs = dict(n_components = 2, \n",
    "                                   init = None, \n",
    "                                   solver = \"cd\",\n",
    "                                   alpha_W = 0,  # default\n",
    "                                   alpha_H = 0,  # default\n",
    "                                   l1_ratio = 0,  # default\n",
    "                                   max_iter = 100000\n",
    "                                    ))\n",
    "pca_ax = cluster_plotting([sample_cols[[c for c in list(rename_cols.values()) if \"DMSO\" in c ]]], # list with minimum 1 df\n",
    "                 [s for s in sample_groups if \"DMSO\" in s],         \n",
    "                 [fr\"PCA Clustering 0.04% DMSO-treated pY samples\"],\n",
    "                 [pca_out_d_preimp],\n",
    "                 [slice(5*i,5*(i+1)) for i in range(8)],\n",
    "                 [\"JE6 0m DMSO\", \"JE6 5m DMSO\",\n",
    "                  \"J.TCPTP- 0m DMSO\", \"J.TCPTP- 5m DMSO\",\n",
    "                  \"J.PTPN22- 0m DMSO\", \"J.PTPN22- 5m DMSO\",\n",
    "                  \"J.SHP1- 0m DMSO\", \"J.SHP1- 5m DMSO\"],\n",
    "                 [gh.unpack_list([c[:2] for c in colours])],\n",
    "                 markers = [\"o\",\"^\",\"o\",\"^\",\"o\",\"^\",\"o\",\"^\"],\n",
    "                 cluster = 'PCA',\n",
    "                 markersize=100,\n",
    "                 textdict = dict(fontfamily = \"sans-serif\",\n",
    "                 font = \"Arial\",\n",
    "                 fontweight = \"bold\",\n",
    "                 fontsize = 10),\n",
    "                          square = False,\n",
    "                 pca_kwargs = dict(n_components = 5,\n",
    "                                   whiten = False,\n",
    "                                   svd_solver = \"full\",\n",
    "                                   tol = 0),)\n",
    "\n",
    "uslice = [slice(5*i,5*(i+1)) for i in range(3)] + [slice(15, 19)] + [slice(19+5*i,19+5*(i+1))for i in range(4)]\n",
    "\n",
    "nmf_ax = cluster_plotting([sample_cols[[c for c in list(rename_cols.values()) if \"U0126\" in c]]], # list with minimum 1 df\n",
    "                 [s for s in sample_groups if \"U0126\" in s],         \n",
    "                 [fr\"NMF Clustering 20 $\\mu$M-treated pY samples\"],\n",
    "                 [nmf_out_u_preimp],\n",
    "                 uslice,\n",
    "                 [\"JE6 0m U0126\", \"JE6 5m U0126\",\n",
    "                  \"J.TCPTP- 0m U0126\", \"J.TCPTP- 5m U0126\",\n",
    "                  \"J.PTPN22- 0m U0126\", \"J.PTPN22- 5m U0126\",\n",
    "                  \"J.SHP1- 0m U0126\", \"J.SHP1- 5m U0126\"],\n",
    "                 [gh.unpack_list([c[:2] for c in colours])],\n",
    "                 markers = [\"o\",\"^\",\"o\",\"^\",\"o\",\"^\",\"o\",\"^\"],\n",
    "                 cluster = 'NMF',\n",
    "                 markersize=100,\n",
    "                 textdict = dict(fontfamily = \"sans-serif\",\n",
    "                 font = \"Arial\",\n",
    "                 fontweight = \"bold\",\n",
    "                 fontsize = 10),\n",
    "                          square = False,\n",
    "                 nmf_kwargs = dict(n_components = 2, \n",
    "                                   init = None, \n",
    "                                   solver = \"cd\",\n",
    "                                   alpha_W = 0,  # default\n",
    "                                   alpha_H = 0,  # default\n",
    "                                   l1_ratio = 0,  # default\n",
    "                                   max_iter = 100000\n",
    "                                    ))\n",
    "pca_ax = cluster_plotting([sample_cols[[c for c in list(rename_cols.values()) if \"U0126\" in c]]], # list with minimum 1 df\n",
    "                 [s for s in sample_groups if \"U0126\" in s],         \n",
    "                 [fr\"PCA Clustering 20 $\\mu$M-treated pY samples\"],\n",
    "                 [pca_out_u_preimp],\n",
    "                 uslice,\n",
    "                 [\"JE6 0m U0126\", \"JE6 5m U0126\",\n",
    "                  \"J.TCPTP- 0m U0126\", \"J.TCPTP- 5m U0126\",\n",
    "                  \"J.PTPN22- 0m U0126\", \"J.PTPN22- 5m U0126\",\n",
    "                  \"J.SHP1- 0m U0126\", \"J.SHP1- 5m U0126\"],\n",
    "                 [gh.unpack_list([c[:2] for c in colours])],\n",
    "                 markers = [\"o\",\"^\",\"o\",\"^\",\"o\",\"^\",\"o\",\"^\"],\n",
    "                 cluster = 'PCA',\n",
    "                 markersize=100,\n",
    "                 textdict = dict(fontfamily = \"sans-serif\",\n",
    "                 font = \"Arial\",\n",
    "                 fontweight = \"bold\",\n",
    "                 fontsize = 10),\n",
    "                          square = False,\n",
    "                 pca_kwargs = dict(n_components = 5,\n",
    "                                   whiten = False,\n",
    "                                   svd_solver = \"full\",\n",
    "                                   tol = 0),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Need to calculate FCs and q-values between the following groups:\n",
    "\n",
    "group_inds = {\"JE6 0m DMSO\" : [2,3,4,5,6],\n",
    "              \"JE6 5m DMSO\" : [7,8,9,10,11],\n",
    "              \"JE6 0m U0126\" : [12,13,14,15,16],\n",
    "              \"JE6 5m U0126\" : [17,18,19,20,21],\n",
    "              \"TCP 0m DMSO\"  : [22,23,24,25,26],\n",
    "              \"TCP 5m DMSO\"  : [27,28,29,30,31],\n",
    "              \"TCP 0m U0126\" : [32,33,34,35,36],\n",
    "              \"TCP 5m U0126\" : [37,38,39,40],\n",
    "              \"N22 0m DMSO\"  : [41,42,43,44,45],\n",
    "              \"N22 5m DMSO\"  : [46,47,48,49,50],\n",
    "              \"N22 0m U0126\" : [51,52,53,54,55],\n",
    "              \"N22 5m U0126\" : [56,57,58,59,60],\n",
    "              \"SHP 0m DMSO\"  : [61,62,63,64,65],\n",
    "              \"SHP 5m DMSO\"  : [66,67,68,69,70],\n",
    "              \"SHP 0m U0126\" : [71,72,73,74,75],\n",
    "              \"SHP 5m U0126\" : [76,77,78,79,80]\n",
    "             }\n",
    "\n",
    "comparisons = [(\"TCP 0m DMSO\", \"JE6 0m DMSO\", \"basald\"),\n",
    "               (\"N22 0m DMSO\", \"JE6 0m DMSO\", \"basald\"),\n",
    "               (\"SHP 0m DMSO\", \"JE6 0m DMSO\", \"basald\"), # Basal state/JE6 comps DMSO\n",
    "               (\"TCP 5m DMSO\", \"JE6 5m DMSO\", \"stimd\"),\n",
    "               (\"N22 5m DMSO\", \"JE6 5m DMSO\", \"stimd\"),\n",
    "               (\"SHP 5m DMSO\", \"JE6 5m DMSO\", \"stimd\"), # Stim state/JE6 comps DMSO\n",
    "               (\"JE6 5m DMSO\", \"JE6 0m DMSO\", \"5m0md\"),\n",
    "               (\"TCP 5m DMSO\", \"TCP 0m DMSO\", \"5m0md\"),\n",
    "               (\"N22 5m DMSO\", \"N22 0m DMSO\", \"5m0md\"),\n",
    "               (\"SHP 5m DMSO\", \"SHP 0m DMSO\", \"5m0md\"), # 5m Stims DMSO\n",
    "               (\"TCP 0m U0126\", \"JE6 0m U0126\", \"basalu\"),\n",
    "               (\"N22 0m U0126\", \"JE6 0m U0126\", \"basalu\"),\n",
    "               (\"SHP 0m U0126\", \"JE6 0m U0126\", \"basalu\"), # Basal state/JE6 comps DMSO\n",
    "               (\"TCP 5m U0126\", \"JE6 5m U0126\", \"stimu\"),\n",
    "               (\"N22 5m U0126\", \"JE6 5m U0126\", \"stimu\"),\n",
    "               (\"SHP 5m U0126\", \"JE6 5m U0126\", \"stimu\"), # Stim state/JE6 comps DMSO\n",
    "               (\"JE6 0m U0126\", \"JE6 0m DMSO\", \"basalpm\"),\n",
    "               (\"TCP 0m U0126\", \"TCP 0m DMSO\", \"basalpm\"),\n",
    "               (\"N22 0m U0126\", \"N22 0m DMSO\", \"basalpm\"),\n",
    "               (\"SHP 0m U0126\", \"SHP 0m DMSO\", \"basalpm\"), # Basal state +/- U0126\n",
    "               (\"JE6 5m U0126\", \"JE6 5m DMSO\", \"stimpm\"),\n",
    "               (\"TCP 5m U0126\", \"TCP 5m DMSO\", \"stimpm\"),\n",
    "               (\"N22 5m U0126\", \"N22 5m DMSO\", \"stimpm\"),\n",
    "               (\"SHP 5m U0126\", \"SHP 5m DMSO\", \"stimpm\"), # Stim state +/- U0126\n",
    "               (\"JE6 5m U0126\", \"JE6 0m U0126\", \"5m0mu\"),\n",
    "               (\"TCP 5m U0126\", \"TCP 0m U0126\", \"5m0mu\"),\n",
    "               (\"N22 5m U0126\", \"N22 0m U0126\", \"5m0mu\"),\n",
    "               (\"SHP 5m U0126\", \"SHP 0m U0126\", \"5m0mu\"), # 5m stims U0126\n",
    "               ]\n",
    "\n",
    "\n",
    "hist_labels = [[\"0 min\"], [\"5 min\"],[\"0 min\"], [\"5 min\"],\n",
    "               [\"0 min\"], [\"5 min\"],[\"0 min\"], [\"5 min\"],\n",
    "               [\"0 min\"], [\"5 min\"],[\"0 min\"], [\"5 min\"], \n",
    "               [\"0 min\"], [\"5 min\"],[\"0 min\"], [\"5 min\"]]\n",
    "hist_subgroups = [r\"$0.04\\%$ DMSO\", r\"$20\\mu$M U0126\",\n",
    "                  r\"$0.04\\%$ DMSO\", r\"$20\\mu$M U0126\",\n",
    "                  r\"$0.04\\%$ DMSO\", r\"$20\\mu$M U0126\",\n",
    "                  r\"$0.04\\%$ DMSO\", r\"$20\\mu$M U0126\"]\n",
    "hist_groups = [\"JE6\", \"J.TCPTP-\", \"J.PTPN22-\", \"J.SHP1-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_slices = [slice(value[0]-2,value[-1]-1) for key, value in group_inds.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values just because\n",
    "if os.path.exists(\"tmp/imputed.csv\"):\n",
    "    imputed_df = pd.read_csv(\"tmp/imputed.csv\")\n",
    "    hist_data = [list(np.log2(file[v]).to_numpy()) for k, v in rename_cols.items() if \" R\" in v]\n",
    "    imputed = [list(np.log2(imputed_df[v]).to_numpy()) for k, v in rename_cols.items() if \" R\" in v]\n",
    "    for col in rename_cols.values() :\n",
    "        if \"O R\" in col or \"6 R\" in col:\n",
    "            file[col] = np.log2(imputed_df[col])\n",
    "else:\n",
    "    imputer = MissForest(random_state = 69)\n",
    "    imputed_data = imputer.fit_transform(file[colnames].to_numpy().transpose())\n",
    "    imputed_df = pd.DataFrame(imputed_data.transpose(), columns = [col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col])\n",
    "    imputed_df.to_csv(\"tmp/imputed.csv\")\n",
    "    hist_data = [list(np.log2(file[v]).to_numpy()) for k, v in rename_cols.items() if \" R\" in v]\n",
    "    imputed = [list(np.log2(imputed[v]).to_numpy()) for k, v in rename_cols.items() if \" R\" in v]\n",
    "    for col in rename_cols.values() :\n",
    "        if \"O R\" in col or \"6 R\" in col:\n",
    "            file[col] = np.log2(imputed_df[col])\n",
    "\n",
    "imputed = [[imputed[i][j] for j in range(len(imputed[i])) if hist_data[i][j] != hist_data[i][j]] for i in range(len(imputed))]\n",
    "hist_data = [[item for item in col if item == item] for col in hist_data]\n",
    "\n",
    "imputed = [imputed[s] for s in group_slices]\n",
    "hist_data = [hist_data[s] for s in group_slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_matrix(a_matrix, # Assumes no headers, remove them\n",
    "                 ylabels,\n",
    "                xlabels,\n",
    "                bins = 33,\n",
    "               imputed = None, # Must have the same shape as a_matrix, minus the number of points\n",
    "               ymax = 150,\n",
    "                xlims = (15,35),\n",
    "                ygroup_size = None, # Should be top to bottom\n",
    "               y_groups = None,     # Should be top to bottom\n",
    "                title = \"Lick mah balls\",\n",
    "                fontdict = dict(fontfamily = \"sans-serif\",\n",
    "                                font = \"Arial\",\n",
    "                                fontstyle = \"normal\"),\n",
    "                primary = (\"lavendar\", \"mediumpurple\"),\n",
    "                secondary = (\"pink\", \"mediumvioletred\"),\n",
    "                figsize = (16,12),\n",
    "                savefile = \"histogram_matrix.pdf\",\n",
    "                missing = [],\n",
    "               ):\n",
    "    font = font_manager.FontProperties(family='sans-serif',\n",
    "                                   style='normal', size=8)\n",
    "    fig, ax = plt.subplots(len(ylabels), len(xlabels),\n",
    "                           figsize = figsize)\n",
    "    # This is the number of rows in the figure\n",
    "    for i in range(len(ylabels)):\n",
    "        # This is the number of cols in the figure,\n",
    "        # should equal len(xlabels)\n",
    "        for j in range(len(xlabels)):\n",
    "            ax[i][j].set_xlim(*xlims)\n",
    "            ax[i][j].set_ylim(0,ymax)\n",
    "            if (i,j) not in missing:\n",
    "                hist_med = sh.median(a_matrix[i][j])\n",
    "                ax[i][j].hist(a_matrix[i][j], bins = bins,\n",
    "                              color = primary[0], edgecolor = primary[1],\n",
    "                              label = fr\"$n={len([d for d in a_matrix[i][j] if d == d])}$\")\n",
    "                ax[i][j].plot([hist_med, hist_med], [0,ymax], color = primary[0],\n",
    "                             linestyle = \":\",\n",
    "                             label = fr\"$n_{{MED}}={hist_med:.2f}$\")\n",
    "                if imputed != None:\n",
    "                    imp_med = sh.median(imputed[i][j])\n",
    "                    ax[i][j].hist(imputed[i][j], bins = bins,\n",
    "                                  color = secondary[0], edgecolor = secondary[1],\n",
    "                                  label = fr\"$i={len([d for d in imputed[i][j] if d == d])}$\")\n",
    "                    ax[i][j].plot([imp_med, imp_med], [0,ymax], color = secondary[0],\n",
    "                                 linestyle = \":\",\n",
    "                                 label = fr\"$i_{{MED}}={imp_med:.2f}$\")\n",
    "\n",
    "                # Lifestyle choices \n",
    "                ax[i][j].legend(loc = \"upper right\", prop=font)\n",
    "            if j != 0:\n",
    "                ytcks = list(ax[i][j].get_yticks()[1:])\n",
    "                ax[i][j].set_yticks(ytcks)\n",
    "                ax[i][j].set_yticklabels([\"\" for _ in range(len(ytcks))])\n",
    "            elif (i,j) in missing:\n",
    "                ytcks = [0] + list(ax[0][0].get_yticks())\n",
    "                ax[i][j].set_yticks(ytcks)\n",
    "                mph.update_ticks(ax[i][j],\n",
    "                                 which = \"y\",\n",
    "                                  fontdict = {\"fontfamily\" : \"sans-serif\",\n",
    "                                   \"font\" : \"Arial\",\n",
    "                                   #\"ha\" : \"center\",\n",
    "                                   \"fontweight\" : \"bold\",\n",
    "                                   \"fontsize\" : \"8\"})\n",
    "                ax[i][j].set_ylabel(ylabels[i], fontweight = \"bold\", fontsize= \"8\",\n",
    "                                         **fontdict)\n",
    "            else:\n",
    "                mph.update_ticks(ax[i][j],\n",
    "                                 which = \"y\",\n",
    "                                  fontdict = {\"fontfamily\" : \"sans-serif\",\n",
    "                                   \"font\" : \"Arial\",\n",
    "                                   #\"ha\" : \"center\",\n",
    "                                   \"fontweight\" : \"bold\",\n",
    "                                   \"fontsize\" : \"8\"})\n",
    "                ax[i][j].set_ylabel(ylabels[i], fontweight = \"bold\", fontsize= \"8\",\n",
    "                                         **fontdict)\n",
    "            if i != len(ylabels)-1:\n",
    "                xtcks = list(ax[i][j].get_xticks()[1:])\n",
    "                ax[i][j].set_xticks(xtcks)\n",
    "                ax[i][j].set_xticklabels([\"\" for _ in range(len(xtcks))])\n",
    "            elif (i,j) in missing:\n",
    "                xtcks = [xlims[0]] + list(ax[0][0].get_xticks())\n",
    "                ax[i][j].set_xticks(xtcks)\n",
    "                mph.update_ticks(ax[i][j],\n",
    "                                 which = \"x\",\n",
    "                                  fontdict = {\"fontfamily\" : \"sans-serif\",\n",
    "                                   \"font\" : \"Arial\",\n",
    "                                   #\"ha\" : \"center\",\n",
    "                                   \"fontweight\" : \"bold\",\n",
    "                                   \"fontsize\" : \"8\"})\n",
    "                ax[i][j].spines[:].set_visible(False)\n",
    "            else:\n",
    "                mph.update_ticks(ax[i][j],\n",
    "                                     which = \"x\",\n",
    "                                      fontdict = {\"fontfamily\" : \"sans-serif\",\n",
    "                                       \"font\" : \"Arial\",\n",
    "                                       #\"ha\" : \"center\",\n",
    "                                       \"fontweight\" : \"bold\",\n",
    "                                       \"fontsize\" : \"8\"})\n",
    "                ax[i][j].set_xlabel(xlabels[j], fontweight = \"bold\", fontsize = 8,\n",
    "                                        **fontdict)\n",
    "    # Get the spacing correct\n",
    "    plt.subplots_adjust(wspace = 0.1, hspace = 0.1)\n",
    "    if y_groups != None and ygroup_size != None:\n",
    "        # Place the text strings where they should go\n",
    "        # These are in figure units, not axes units\n",
    "        fig.text(0.02,0.45, title, rotation = 90,\n",
    "                fontweight = \"bold\", fontsize = \"14\",\n",
    "                 **fontdict)\n",
    "        # Loop over the group sizes in reverse order\n",
    "        displacement = 0\n",
    "        index = 0\n",
    "        for gsize in [ygroup_size[-i] for i in range(len(ygroup_size))]:\n",
    "            # Calculate displacement for group\n",
    "            center = (gsize/sum(ygroup_size))/2 + displacement\n",
    "            fig.text(0.05, center, y_groups[index], rotation = 90,\n",
    "                     va = \"center\", ha = \"center\",\n",
    "                     fontweight = \"bold\", fontsize = \"14\",\n",
    "                     **fontdict)\n",
    "            displacement += gsize/sum(ygroup_size)\n",
    "            index += 1\n",
    "    plt.savefig(savefile)\n",
    "    plt.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_savefiles = [\"figs/je6/qc/log2_replicate_distr_je6.pdf\",\n",
    "                  \"figs/jtcptp/qc/log2_replicate_distr_tcp.pdf\",\n",
    "                  \"figs/jptpn22/qc/log2_replicate_distr_n22.pdf\",\n",
    "                  \"figs/jshp1/qc/log2_replicate_distr_shp.pdf\"]\n",
    "\n",
    "missing_reps = [[], [(3,4)], [],[]]\n",
    "for i in range(4):\n",
    "    \n",
    "    hist_matrix(hist_data[4*i:4*(i+1)], # Assumes no headers, remove them\n",
    "            [\"0 min\", \"5 min\", \"0 min\", \"5 min\"],\n",
    "            [\"R1\" , \"R2\", \"R3\", \"R4\", \"R5\"],\n",
    "            bins = 33,\n",
    "            imputed = imputed[:4], # Must have the same shape as a_matrix, minus the number of points\n",
    "            ymax = 150,\n",
    "            xlims = (10,35),\n",
    "            ygroup_size = [2,2], # Should be top to bottom\n",
    "            y_groups = [\"0.04% DMSO\", fr\"20 $\\mu$M U0126\"],     # Should be top to bottom\n",
    "            title = hist_groups[i],\n",
    "            fontdict = dict(fontfamily = \"sans-serif\",\n",
    "                                font = \"Arial\",\n",
    "                                fontstyle = \"normal\"),\n",
    "            primary = (\"lavender\", \"mediumpurple\"),\n",
    "            secondary = (\"pink\", \"mediumvioletred\"),\n",
    "            figsize = (16,12),\n",
    "            savefile = hist_savefiles[i],\n",
    "            missing = missing_reps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_groups = [gh.list_to_str(c.split(\" \")[:3], \n",
    "                                delimiter = \" \",\n",
    "                                newline = False) for c in list(rename_cols.values()) if \" R\" in c]\n",
    "\n",
    "sample_groups = uniq_list(sample_groups)\n",
    "\n",
    "colnames = [c for c in list(rename_cols.values()) if \" R\" in c]\n",
    "abundances = [list(row) for row in file[colnames].astype(float).to_numpy()]\n",
    "\n",
    "sample_cols = gh.transpose(*abundances)\n",
    "sample_cols = [col for col in sample_cols if not all([item != item for item in col])]\n",
    "sample_cols = gh.transpose(*sample_cols)\n",
    "sample_cols = [row for row in sample_cols if not any([item != item for item in row])]\n",
    "sample_cols = pd.DataFrame(sample_cols, columns = [c for c in list(rename_cols.values()) if \" R\" in c])\n",
    "sample_cols = sample_cols.apply(np.log2)\n",
    "\n",
    "pca_out_d_postimp = \"figs/qc/postimp/dmso_pca.pdf\"\n",
    "nmf_out_d_postimp = \"figs/qc/postimp/dmso_nmf.pdf\"\n",
    "\n",
    "pca_out_u_postimp = \"figs/qc/postimp/u0126_pca.pdf\"\n",
    "nmf_out_u_postimp = \"figs/qc/postimp/u0126_nmf.pdf\"\n",
    "\n",
    "\n",
    "\n",
    "multi_reg_lineplot(sample_cols, groups = sample_groups,\n",
    "                   labels = sample_groups, log2_trans = False,\n",
    "                   savefile = \"figs/qc/postimp/multi_linreg.pdf\")\n",
    "\n",
    "nmf_ax = cluster_plotting([sample_cols[[c for c in list(rename_cols.values()) if \"DMSO\" in c]]], # list with minimum 1 df\n",
    "                 [s for s in sample_groups if \"DMSO\" in s],         \n",
    "                 [fr\"NMF Clustering 0.04% DMSO-treated pY samples\"],\n",
    "                 [nmf_out_d_postimp],\n",
    "                 [slice(5*i,5*(i+1)) for i in range(8)],\n",
    "                 [\"JE6 0m DMSO\", \"JE6 5m DMSO\",\n",
    "                  \"J.TCPTP- 0m DMSO\", \"J.TCPTP- 5m DMSO\",\n",
    "                  \"J.PTPN22- 0m DMSO\", \"J.PTPN22- 5m DMSO\",\n",
    "                  \"J.SHP1- 0m DMSO\", \"J.SHP1- 5m DMSO\"],\n",
    "                 [gh.unpack_list([c[:2] for c in colours])],\n",
    "                 markers = [\"o\",\"^\",\"o\",\"^\",\"o\",\"^\",\"o\",\"^\"],\n",
    "                 cluster = 'NMF',\n",
    "                 markersize=100,\n",
    "                 textdict = dict(fontfamily = \"sans-serif\",\n",
    "                 font = \"Arial\",\n",
    "                 fontweight = \"bold\",\n",
    "                 fontsize = 10),\n",
    "                          square = False,\n",
    "                 nmf_kwargs = dict(n_components = 3, \n",
    "                                   init = None, \n",
    "                                   solver = \"cd\",\n",
    "                                   alpha_W = 0,  # default\n",
    "                                   alpha_H = 0,  # default\n",
    "                                   l1_ratio = 0,  # default\n",
    "                                   max_iter = 100000\n",
    "                                    ))\n",
    "pca_ax = cluster_plotting([sample_cols[[c for c in list(rename_cols.values()) if \"DMSO\" in c ]]], # list with minimum 1 df\n",
    "                 [s for s in sample_groups if \"DMSO\" in s],         \n",
    "                 [fr\"PCA Clustering 0.04% DMSO-treated pY samples\"],\n",
    "                 [pca_out_d_postimp],\n",
    "                 [slice(5*i,5*(i+1)) for i in range(8)],\n",
    "                 [\"JE6 0m DMSO\", \"JE6 5m DMSO\",\n",
    "                  \"J.TCPTP- 0m DMSO\", \"J.TCPTP- 5m DMSO\",\n",
    "                  \"J.PTPN22- 0m DMSO\", \"J.PTPN22- 5m DMSO\",\n",
    "                  \"J.SHP1- 0m DMSO\", \"J.SHP1- 5m DMSO\"],\n",
    "                 [gh.unpack_list([c[:2] for c in colours])],\n",
    "                 markers = [\"o\",\"^\",\"o\",\"^\",\"o\",\"^\",\"o\",\"^\"],\n",
    "                 cluster = 'PCA',\n",
    "                 markersize=100,\n",
    "                 textdict = dict(fontfamily = \"sans-serif\",\n",
    "                 font = \"Arial\",\n",
    "                 fontweight = \"bold\",\n",
    "                 fontsize = 10),\n",
    "                          square = False,\n",
    "                 pca_kwargs = dict(n_components = 5,\n",
    "                                   whiten = False,\n",
    "                                   svd_solver = \"full\",\n",
    "                                   tol = 0),)\n",
    "\n",
    "uslice = [slice(5*i,5*(i+1)) for i in range(3)] + [slice(15, 19)] + [slice(19+5*i,19+5*(i+1))for i in range(4)]\n",
    "\n",
    "nmf_ax = cluster_plotting([sample_cols[[c for c in list(rename_cols.values()) if \"U0126\" in c]]], # list with minimum 1 df\n",
    "                 [s for s in sample_groups if \"U0126\" in s],         \n",
    "                 [fr\"NMF Clustering 20 $\\mu$M-treated pY samples\"],\n",
    "                 [nmf_out_u_postimp],\n",
    "                 uslice,\n",
    "                 [\"JE6 0m U0126\", \"JE6 5m U0126\",\n",
    "                  \"J.TCPTP- 0m U0126\", \"J.TCPTP- 5m U0126\",\n",
    "                  \"J.PTPN22- 0m U0126\", \"J.PTPN22- 5m U0126\",\n",
    "                  \"J.SHP1- 0m U0126\", \"J.SHP1- 5m U0126\"],\n",
    "                 [gh.unpack_list([c[:2] for c in colours])],\n",
    "                 markers = [\"o\",\"^\",\"o\",\"^\",\"o\",\"^\",\"o\",\"^\"],\n",
    "                 cluster = 'NMF',\n",
    "                 markersize=100,\n",
    "                 textdict = dict(fontfamily = \"sans-serif\",\n",
    "                 font = \"Arial\",\n",
    "                 fontweight = \"bold\",\n",
    "                 fontsize = 10),\n",
    "                          square = False,\n",
    "                 nmf_kwargs = dict(n_components = 3, \n",
    "                                   init = None, \n",
    "                                   solver = \"cd\",\n",
    "                                   alpha_W = 0,  # default\n",
    "                                   alpha_H = 0,  # default\n",
    "                                   l1_ratio = 0,  # default\n",
    "                                   max_iter = 100000\n",
    "                                    ))\n",
    "pca_ax = cluster_plotting([sample_cols[[c for c in list(rename_cols.values()) if \"U0126\" in c]]], # list with minimum 1 df\n",
    "                 [s for s in sample_groups if \"U0126\" in s],         \n",
    "                 [fr\"PCA Clustering 20 $\\mu$M-treated pY samples\"],\n",
    "                 [pca_out_u_postimp],\n",
    "                 uslice,\n",
    "                 [\"JE6 0m U0126\", \"JE6 5m U0126\",\n",
    "                  \"J.TCPTP- 0m U0126\", \"J.TCPTP- 5m U0126\",\n",
    "                  \"J.PTPN22- 0m U0126\", \"J.PTPN22- 5m U0126\",\n",
    "                  \"J.SHP1- 0m U0126\", \"J.SHP1- 5m U0126\"],\n",
    "                 [gh.unpack_list([c[:2] for c in colours])],\n",
    "                 markers = [\"o\",\"^\",\"o\",\"^\",\"o\",\"^\",\"o\",\"^\"],\n",
    "                 cluster = 'PCA',\n",
    "                 markersize=100,\n",
    "                 textdict = dict(fontfamily = \"sans-serif\",\n",
    "                 font = \"Arial\",\n",
    "                 fontweight = \"bold\",\n",
    "                 fontsize = 10),\n",
    "                          square = False,\n",
    "                 pca_kwargs = dict(n_components = 5,\n",
    "                                   whiten = False,\n",
    "                                   svd_solver = \"full\",\n",
    "                                   tol = 0),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for pi0: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4049/2207630204.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} Fold change\"] = fc[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} Fold change\"] = fc[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} Fold change\"] = fc[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} Fold change\"] = fc[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} Fold change\"] = fc[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} Fold change\"] = fc[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} Fold change\"] = fc[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} Fold change\"] = fc[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} Fold change\"] = fc[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} Fold change\"] = fc[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
      "/tmp/ipykernel_4049/2207630204.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n"
     ]
    }
   ],
   "source": [
    "# Get the rows of file, which are the imputed values\n",
    "rows = [list(row) for row in file.to_numpy()]\n",
    "\n",
    "fc = [[sh.mean([row[x] for x in group_inds[c[0]]]) - sh.mean([row[x] for x in group_inds[c[1]]]) for row in rows]\n",
    "      for c in comparisons]\n",
    "\n",
    "tests = [[sh.TTest([row[x] for x in group_inds[c[0]]],\n",
    "                   [row[x] for x in group_inds[c[1]]],\n",
    "                   test_type = \"w\",\n",
    "                   threshold = 3,\n",
    "                   labels = False).output[0][\"pvalue\"][0] for row in rows] for c in comparisons]\n",
    "unpacked_ps = gh.unpack_list(tests)\n",
    "global_q = list(sh.storey(unpacked_ps, pi0=1)[\"qvalue\"].astype(float))\n",
    "packed_qs = [global_q[len(fc[0])*i:len(fc[0])*(i+1)] for i in range(len(fc))]\n",
    "\n",
    "for i in range(len(tests)):\n",
    "    file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} Fold change\"] = fc[i]\n",
    "for i in range(len(tests)):\n",
    "    file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} p-value\"] = tests[i]\n",
    "for i in range(len(tests)):\n",
    "    file[f\"{comparisons[i][0]} vs {comparisons[i][1]} {comparisons[i][2]} q-value\"] = packed_qs[i]\n",
    "file = file[file[\"Gene name\"].notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gct_heads(headers, delim = [\"balls\"], pos= 0):\n",
    "    h = [head for head in headers if any([d in head for d in delim])]\n",
    "    newh = []\n",
    "    for head in h:\n",
    "        for d in delim:\n",
    "            if len(head.split(d)) > 1:\n",
    "                newh.append(head.split(d)[pos])\n",
    "    print(uniq_list(newh))\n",
    "    return uniq_list(newh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TCP 0m DMSO vs JE6 0m DMSO ', 'N22 0m DMSO vs JE6 0m DMSO ', 'SHP 0m DMSO vs JE6 0m DMSO ', 'TCP 5m DMSO vs JE6 5m DMSO ', 'N22 5m DMSO vs JE6 5m DMSO ', 'SHP 5m DMSO vs JE6 5m DMSO ']\n",
      "['JE6 5m DMSO vs JE6 0m DMSO ', 'TCP 5m DMSO vs TCP 0m DMSO ', 'N22 5m DMSO vs N22 0m DMSO ', 'SHP 5m DMSO vs SHP 0m DMSO ']\n",
      "['TCP 0m U0126 vs JE6 0m U0126 ', 'N22 0m U0126 vs JE6 0m U0126 ', 'SHP 0m U0126 vs JE6 0m U0126 ', 'TCP 5m U0126 vs JE6 5m U0126 ', 'N22 5m U0126 vs JE6 5m U0126 ', 'SHP 5m U0126 vs JE6 5m U0126 ']\n",
      "['JE6 5m U0126 vs JE6 0m U0126 ', 'TCP 5m U0126 vs TCP 0m U0126 ', 'N22 5m U0126 vs N22 0m U0126 ', 'SHP 5m U0126 vs SHP 0m U0126 ']\n",
      "['JE6 0m U0126 vs JE6 0m DMSO ', 'TCP 0m U0126 vs TCP 0m DMSO ', 'N22 0m U0126 vs N22 0m DMSO ', 'SHP 0m U0126 vs SHP 0m DMSO ', 'JE6 5m U0126 vs JE6 5m DMSO ', 'TCP 5m U0126 vs TCP 5m DMSO ', 'N22 5m U0126 vs N22 5m DMSO ', 'SHP 5m U0126 vs SHP 5m DMSO ']\n"
     ]
    }
   ],
   "source": [
    "# Write GCT Files\n",
    "\n",
    "groups = [[\"Flanking Sequence\", \"basald\", \"stimd\"],\n",
    "          [\"Flanking Sequence\",\"5m0md\"],\n",
    "          [\"Flanking Sequence\",\"basalu\", \"stimu\"],\n",
    "          [\"Flanking Sequence\",\"5m0mu\"],\n",
    "          [\"Flanking Sequence\",\"basalpm\", \"stimpm\"]\n",
    "         ]\n",
    "\n",
    "# Do this in the same groups as the Venn Diagrams\n",
    "f_cols = list(file.columns)\n",
    "f_cols_1flank = [item for item in f_cols if item not in [\"Flanking Sequence (2)\", \"Flanking Sequence (1)\"]]\n",
    "flank_cols = [f_cols.index(head) for head in f_cols if \"Flanking Sequence\" in head]\n",
    "f_as_list = [list(row) for row in file.to_numpy()]\n",
    "f_as_list = [f_cols_1flank] + dup_flanks_matrix(f_as_list, flank_cols)\n",
    "f_as_list = gh.transpose(*f_as_list)\n",
    "# Filter into the group matrices\n",
    "f_split = [[c for c in f_as_list if any([sep in c[0] for sep in g]) and \"p-value\" not in c[0]]\n",
    "           for g in groups]\n",
    "f_split = [gh.transpose(*matr) for matr in f_split]\n",
    "# Comparisons are always between FC and q: (len(heads)-1)/2\n",
    "f_flank = [[[f\"{row[0]}-p\"] + [ptmsea_transform(row[len(row)//2 + i+1],\n",
    "                                    row[i+1]) for i in range(len(row)//2)]\n",
    "           for row in matr[1:]]\n",
    "           for matr in f_split]\n",
    "\n",
    "f_flank = [sorted(matr, key = lambda x: x[0]) for matr in f_flank]\n",
    "f_flank = [[row for row in matr if row[0] != \"nan-p\"] for matr in f_flank]\n",
    "\n",
    "gct_inds = [[row[0] for row in matr] for matr in f_flank]\n",
    "gct_vals = [[row[1:] for row in matr] for matr in f_flank]\n",
    "gct_heads = [make_gct_heads(f_split[i][0], groups[i][1:]) for i in range(len(f_split))]\n",
    "gct_outs = [\"./figs/ptmsea_output/ko_wt_dmso/qvalue.gct\",\n",
    "            \"./figs/ptmsea_output/timecourse_wo_u0126/qvalue.gct\",\n",
    "            \"./figs/ptmsea_output/ko_wt_u0126/qvalue.gct\",\n",
    "             \"./figs/ptmsea_output/timecourse_w_u0126/qvalue.gct\",\n",
    "            \"./figs/ptmsea_output/pm_u0126/qvalue.gct\",\n",
    "            ]\n",
    "\n",
    "for i in range(len(f_flank)):\n",
    "    write_gct_file(gct_inds[i], gct_vals[i], gct_heads[i], gct_outs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#gsf.imp_main(\"./figs/ptmsea_output\", \"qvalue\", \"./r_scripts/ssGSEA2.0.R\", \n",
    "#             \"./database/ptm.sig.db.all.flanking.human.v2.0.0.gmt\")\n",
    "#file.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mso.imp_main(\"./figs/ptmsea_output\", \"output-combined.gct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_pepplots(peptide_list, path = \"outputs/graphics\",\n",
    "                      subset = [\"all\"], exclude = [], comparisons = [],\n",
    "                      foldchange_group = None,\n",
    "                      global_max = 6):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if global_max == None:\n",
    "        max_val = math.ceil(max([max(p.vals) for p in peptide_list]))\n",
    "        min_val = math.floor(min([min(p.vals) for p in peptide_list]))\n",
    "        hm_max = math.ceil(max([max(p.means) for p in peptide_list]))\n",
    "        hm_min = math.floor(min([min(p.means) for p in peptide_list]))\n",
    "        hm = [-max([abs(hm_min), abs(hm_max)]), max([abs(hm_min), abs(hm_max)])]\n",
    "        fc_max = math.ceil(max([max(p.fc) for p in peptide_list]))\n",
    "        fc_min = math.floor(min([min(p.fc) for p in peptide_list]))\n",
    "        fc = [-max([abs(fc_min), abs(fc_max)]), max([abs(fc_min), abs(fc_max)])]\n",
    "    else:\n",
    "        fc = [-global_max, global_max]\n",
    "    for p in peptide_list:\n",
    "        if not os.path.exists(f\"{path}/{p.gene.lower()}/heatmaps\"):\n",
    "            os.makedirs(f\"{path}/{p.gene.lower()}/heatmaps\")\n",
    "        p.heatmap(d_type = \"foldchange\",\n",
    "                  path = f\"{path}/{p.gene.lower()}/heatmaps/\", \n",
    "                  maxs = fc,\n",
    "                  subset = subset, exclude = exclude)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_labels = [f\"{c[0]} vs {c[1]} {c[2]}\" for c in comparisons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basal = [Peptide(list(file.iloc[i][[col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col]].astype(float)),\n",
    "                    [col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col],\n",
    "                    sample_groups,\n",
    "                    file.iloc[i]['Modified Sequence'],\n",
    "                    statistics = list(file.iloc[i][[col for col in list(file.columns) if \"q-value\" in col and \"basald\" in col]].astype(float)),\n",
    "                    statistics_headers =[col for col in list(file.columns) if \"q-value\" in col and \"basald\" in col],\n",
    "                    foldchange = list(file.iloc[i][[col for col in list(file.columns) if \"Fold change\" in col and \"basald\" in col]].astype(float)),\n",
    "                    foldchange_headers = [f\"{c[0]} vs {c[1]} {c[2]}\" for c in comparisons if c[2] == \"basald\" ],\n",
    "                    sites = file.iloc[i]['Site'],\n",
    "                    gene = file.iloc[i][\"Gene name\"],\n",
    "                    unique_id = file.iloc[i][\"U_ID\"],\n",
    "                    colours = colours[0][:4] + colours[1][:4] + colours[2][:4] + colours[3][:4],\n",
    "                    markers = [\"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",]) for i in range(len(file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes all 0m comparison plots for - U0126\n",
    "make_all_pepplots(basal, \n",
    "                  comparisons = [f\"{c} q-value\" for c in comp_labels[:3]],\n",
    "                subset = [\"all\"], exclude = [], \n",
    "                 foldchange_group = \"JE6 0m DMSO\",\n",
    "                  path = \"figs/pep_plots/basal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimd = [Peptide(list(file.iloc[i][[col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col]].astype(float)),\n",
    "                    [col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col],\n",
    "                    sample_groups,\n",
    "                    file.iloc[i]['Modified Sequence'],\n",
    "                    statistics = list(file.iloc[i][[col for col in list(file.columns) if \"q-value\" in col and \"stimd\" in col]].astype(float)),\n",
    "                    statistics_headers =[col for col in list(file.columns) if \"q-value\" in col and \"stimd\" in col],\n",
    "                    foldchange = list(file.iloc[i][[col for col in list(file.columns) if \"Fold change\" in col and \"stimd\" in col]].astype(float)),\n",
    "                    foldchange_headers = [f\"{c[0]} vs {c[1]} {c[2]}\" for c in comparisons if c[2] == \"stimd\" ],\n",
    "                    sites = file.iloc[i]['Site'],\n",
    "                    gene = file.iloc[i][\"Gene name\"],\n",
    "                    unique_id = file.iloc[i][\"U_ID\"],\n",
    "                    colours = colours[0][:4] + colours[1][:4] + colours[2][:4] + colours[3][:4],\n",
    "                    markers = [\"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",]) for i in range(len(file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_all_pepplots(stimd, \n",
    "                  comparisons = [f\"{c} q-value\" for c in comp_labels[3:6]],\n",
    "                subset = [\"all\"], exclude = [], \n",
    "                 foldchange_group = \"JE6 5m DMSO\",\n",
    "                  path = \"figs/pep_plots/stim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s50d = [Peptide(list(file.iloc[i][[col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col]].astype(float)),\n",
    "                    [col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col],\n",
    "                    sample_groups,\n",
    "                    file.iloc[i]['Modified Sequence'],\n",
    "                    statistics = list(file.iloc[i][[col for col in list(file.columns) if \"q-value\" in col and \"5m0md\" in col]].astype(float)),\n",
    "                    statistics_headers =[col for col in list(file.columns) if \"q-value\" in col and \"5m0md\" in col],\n",
    "                    foldchange = list(file.iloc[i][[col for col in list(file.columns) if \"Fold change\" in col and \"5m0md\" in col]].astype(float)),\n",
    "                    foldchange_headers = [f\"{c[0]} vs {c[1]} {c[2]}\" for c in comparisons if c[2] == \"5m0md\" ],\n",
    "                    sites = file.iloc[i]['Site'],\n",
    "                    gene = file.iloc[i][\"Gene name\"],\n",
    "                    unique_id = file.iloc[i][\"U_ID\"],\n",
    "                    colours = colours[0][:4] + colours[1][:4] + colours[2][:4] + colours[3][:4],\n",
    "                    markers = [\"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",]) for i in range(len(file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes all 0m to 5m comparions within -U0126 group\n",
    "make_all_pepplots(s50d,\n",
    "                  comparisons =[f\"{c} q-value\" for c in comp_labels[6:10]],\n",
    "                 subset = [\"all\"], exclude = [], \n",
    "                 foldchange_group = \"JE6 0m DMSO\",\n",
    "                 path = \"figs/pep_plots/dmso_timecourse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalu = [Peptide(list(file.iloc[i][[col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col]].astype(float)),\n",
    "                    [col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col],\n",
    "                    sample_groups,\n",
    "                    file.iloc[i]['Modified Sequence'],\n",
    "                    statistics = list(file.iloc[i][[col for col in list(file.columns) if \"q-value\" in col and \"basalu\" in col]].astype(float)),\n",
    "                    statistics_headers =[col for col in list(file.columns) if \"q-value\" in col and \"basalu\" in col],\n",
    "                    foldchange = list(file.iloc[i][[col for col in list(file.columns) if \"Fold change\" in col and \"basalu\" in col]].astype(float)),\n",
    "                    foldchange_headers = [f\"{c[0]} vs {c[1]} {c[2]}\" for c in comparisons if c[2] == \"basalu\" ],\n",
    "                    sites = file.iloc[i]['Site'],\n",
    "                    gene = file.iloc[i][\"Gene name\"],\n",
    "                    unique_id = file.iloc[i][\"U_ID\"],\n",
    "                    colours = colours[0][:4] + colours[1][:4] + colours[2][:4] + colours[3][:4],\n",
    "                    markers = [\"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",]) for i in range(len(file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes all 0m comparison plots for + U0126\n",
    "make_all_pepplots(basalu, \n",
    "                  comparisons = [f\"{c} q-value\" for c in comp_labels[10:13]],\n",
    "                  subset = [\"all\"], exclude = [], \n",
    "                 foldchange_group = \"JE6 0m U0126\",\n",
    "                  path = \"figs/pep_plots/u0126_0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimu = [Peptide(list(file.iloc[i][[col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col]].astype(float)),\n",
    "                    [col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col],\n",
    "                    sample_groups,\n",
    "                    file.iloc[i]['Modified Sequence'],\n",
    "                    statistics = list(file.iloc[i][[col for col in list(file.columns) if \"q-value\" in col and \"stimu\" in col]].astype(float)),\n",
    "                    statistics_headers =[col for col in list(file.columns) if \"q-value\" in col and \"stimu\" in col],\n",
    "                    foldchange = list(file.iloc[i][[col for col in list(file.columns) if \"Fold change\" in col and \"stimu\" in col]].astype(float)),\n",
    "                    foldchange_headers = [f\"{c[0]} vs {c[1]} {c[2]}\" for c in comparisons if c[2] == \"stimu\" ],\n",
    "                    sites = file.iloc[i]['Site'],\n",
    "                    gene = file.iloc[i][\"Gene name\"],\n",
    "                    unique_id = file.iloc[i][\"U_ID\"],\n",
    "                    colours = colours[0][:4] + colours[1][:4] + colours[2][:4] + colours[3][:4],\n",
    "                    markers = [\"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",]) for i in range(len(file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes all 0m comparison plots for + U0126\n",
    "make_all_pepplots(stimu, \n",
    "                  comparisons = [f\"{c} q-value\" for c in comp_labels[13:16]],\n",
    "                  subset = [\"all\"], exclude = [], \n",
    "                 foldchange_group = \"JE6 5m U0126\",\n",
    "                  path = \"figs/pep_plots/u0126_5m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalpm = [Peptide(list(file.iloc[i][[col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col]].astype(float)),\n",
    "                    [col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col],\n",
    "                    sample_groups,\n",
    "                    file.iloc[i]['Modified Sequence'],\n",
    "                    statistics = list(file.iloc[i][[col for col in list(file.columns) if \"q-value\" in col and \"basalpm\" in col]].astype(float)),\n",
    "                    statistics_headers =[col for col in list(file.columns) if \"q-value\" in col and \"basalpm\" in col],\n",
    "                    foldchange = list(file.iloc[i][[col for col in list(file.columns) if \"Fold change\" in col and \"basalpm\" in col]].astype(float)),\n",
    "                    foldchange_headers = [f\"{c[0]} vs {c[1]} {c[2]}\" for c in comparisons if c[2] == \"basalpm\" ],\n",
    "                    sites = file.iloc[i]['Site'],\n",
    "                    gene = file.iloc[i][\"Gene name\"],\n",
    "                    unique_id = file.iloc[i][\"U_ID\"],\n",
    "                    colours = colours[0][:4] + colours[1][:4] + colours[2][:4] + colours[3][:4],\n",
    "                    markers = [\"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",]) for i in range(len(file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes all 0m comparison plots for + U0126\n",
    "make_all_pepplots(basalpm, \n",
    "                  comparisons = [f\"{c} q-value\" for c in comp_labels[16:20]],\n",
    "                  subset = [\"all\"], exclude = [], \n",
    "                 foldchange_group = \"JE6 0m DMSO\",\n",
    "                  path = \"figs/pep_plots/u0126_basal_pm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimpm = [Peptide(list(file.iloc[i][[col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col]].astype(float)),\n",
    "                    [col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col],\n",
    "                    sample_groups,\n",
    "                    file.iloc[i]['Modified Sequence'],\n",
    "                    statistics = list(file.iloc[i][[col for col in list(file.columns) if \"q-value\" in col and \"stimpm\" in col]].astype(float)),\n",
    "                    statistics_headers =[col for col in list(file.columns) if \"q-value\" in col and \"stimpm\" in col],\n",
    "                    foldchange = list(file.iloc[i][[col for col in list(file.columns) if \"Fold change\" in col and \"stimpm\" in col]].astype(float)),\n",
    "                    foldchange_headers = [f\"{c[0]} vs {c[1]} {c[2]}\" for c in comparisons if c[2] == \"stimpm\" ],\n",
    "                    sites = file.iloc[i]['Site'],\n",
    "                    gene = file.iloc[i][\"Gene name\"],\n",
    "                    unique_id = file.iloc[i][\"U_ID\"],\n",
    "                    colours = colours[0][:4] + colours[1][:4] + colours[2][:4] + colours[3][:4],\n",
    "                    markers = [\"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",]) for i in range(len(file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes all 0m comparison plots for + U0126\n",
    "make_all_pepplots(stimpm, \n",
    "                  comparisons = [f\"{c} q-value\" for c in comp_labels[16:20]],\n",
    "                  subset = [\"all\"], exclude = [], \n",
    "                 foldchange_group = \"JE6 5m U0126\",\n",
    "                  path = \"figs/pep_plots/u0126_stim_pm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s50u = [Peptide(list(file.iloc[i][[col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col]].astype(float)),\n",
    "                    [col for col in list(file.columns) if \"O R\" in col or \"6 R\" in col],\n",
    "                    sample_groups,\n",
    "                    file.iloc[i]['Modified Sequence'],\n",
    "                    statistics = list(file.iloc[i][[col for col in list(file.columns) if \"q-value\" in col and \"5m0mu\" in col]].astype(float)),\n",
    "                    statistics_headers =[col for col in list(file.columns) if \"q-value\" in col and \"5m0mu\" in col],\n",
    "                    foldchange = list(file.iloc[i][[col for col in list(file.columns) if \"Fold change\" in col and \"5m0mu\" in col]].astype(float)),\n",
    "                    foldchange_headers = [f\"{c[0]} vs {c[1]} {c[2]}\" for c in comparisons if c[2] == \"5m0mu\" ],\n",
    "                    sites = file.iloc[i]['Site'],\n",
    "                    gene = file.iloc[i][\"Gene name\"],\n",
    "                    unique_id = file.iloc[i][\"U_ID\"],\n",
    "                    colours = colours[0][:4] + colours[1][:4] + colours[2][:4] + colours[3][:4],\n",
    "                    markers = [\"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",\n",
    "                               \"o\", \"o\", \"s\", \"s\",]) for i in range(len(file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes all 0m vs 5m comparison plots for + U0126\n",
    "make_all_pepplots(s50u, \n",
    "                  comparisons = [f\"{c} q-value\" for c in comp_labels[20:24]],\n",
    "                  subset = [\"all\"], exclude = [], \n",
    "                 foldchange_group = \"JE6 0m U0126\",\n",
    "                  path = \"figs/pep_plots/u0126_timecourse\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
